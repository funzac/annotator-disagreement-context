{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "j9EgU0Qco4ot"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gVWp4cXJpXWZ"
   },
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self):\n",
    "        self.setup_paths()\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def setup_paths(self):\n",
    "        self.path_dev = 'dev/'\n",
    "        self.path_train = 'train/'\n",
    "        self.path_output = 'answer/'\n",
    "        self.path_test = 'test/'\n",
    "\n",
    "        for path in [self.path_dev, self.path_train, self.path_test, self.path_output]:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "    def extract_zip_files(self):\n",
    "        # Extract dev.zip\n",
    "        if not os.listdir(self.path_dev):\n",
    "            with ZipFile('dev.zip', 'r') as dev:\n",
    "                dev.extractall(self.path_dev)\n",
    "\n",
    "        # Extract train.zip\n",
    "        if not os.listdir(self.path_train):\n",
    "            with ZipFile('train.zip', 'r') as train:\n",
    "                train.extractall(self.path_train)\n",
    "\n",
    "        if not os.listdir(self.path_test):\n",
    "            with ZipFile('test_hidden_2.zip', 'r') as test:\n",
    "                test.extractall(self.path_test)\n",
    "\n",
    "    def load_tsv_files(self):\n",
    "        languages = os.listdir(self.path_train)\n",
    "        self.logger.info(f\"Found languages: {languages}\")\n",
    "\n",
    "        # Initialize file paths\n",
    "        label_file_paths_train = []\n",
    "        uses_file_paths_train = []\n",
    "        label_file_paths_dev = []\n",
    "        instance_file_paths_dev = []\n",
    "        uses_file_paths_dev = []\n",
    "        instance_file_paths_test = []\n",
    "        uses_file_paths_test = []\n",
    "        label_file_paths_test = []\n",
    "\n",
    "\n",
    "        for lang in languages:\n",
    "            label_file_paths_train.append(f\"{self.path_train}{lang}/labels.tsv\")\n",
    "            uses_file_paths_train.append(f\"{self.path_train}{lang}/uses.tsv\")\n",
    "            label_file_paths_dev.append(f\"{self.path_dev}{lang}/labels.tsv\")\n",
    "            instance_file_paths_dev.append(f\"{self.path_dev}{lang}/instances.tsv\")\n",
    "            uses_file_paths_dev.append(f\"{self.path_dev}{lang}/uses.tsv\")\n",
    "            instance_file_paths_test.append(f\"{self.path_test}{lang}/instances.tsv\")\n",
    "            uses_file_paths_test.append(f\"{self.path_test}{lang}/uses.tsv\")\n",
    "            label_file_paths_test.append(f\"{self.path_test}{lang}/labels.tsv\")\n",
    "\n",
    "        paths = {\n",
    "            'train_labels_list': label_file_paths_train,\n",
    "            'train_uses_list': uses_file_paths_train,\n",
    "            'dev_labels_list': label_file_paths_dev,\n",
    "            'dev_uses_list': uses_file_paths_dev,\n",
    "            'dev_instances_list': instance_file_paths_dev,\n",
    "            'test_uses_list': uses_file_paths_test,\n",
    "            'test_instances_list': instance_file_paths_test,\n",
    "            'test_labels_list': label_file_paths_test\n",
    "        }\n",
    "\n",
    "        data_dict = {key: [] for key in paths.keys()}\n",
    "\n",
    "        for save_path, path_list in paths.items():\n",
    "            for path in path_list:\n",
    "                with open(path, encoding='utf-8') as tsvfile:\n",
    "                    language = path.split('/')[1]\n",
    "                    reader = csv.DictReader(tsvfile, delimiter='\\t',\n",
    "                                         quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "                    for row in reader:\n",
    "                        row['language'] = language\n",
    "                        data_dict[save_path].append(row)\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    def create_merged_dataframes(self, data_dict):\n",
    "        def create_mappings(uses_list):\n",
    "            id2context = {}\n",
    "            id2idx = {}\n",
    "            for row in uses_list:\n",
    "                identifier = row['identifier']\n",
    "                id2context[identifier] = row['context']\n",
    "                id2idx[identifier] = row['indices_target_token']\n",
    "            return id2context, id2idx\n",
    "\n",
    "        # Create mappings\n",
    "        train_id2context, train_id2idx = create_mappings(data_dict['train_uses_list'])\n",
    "        dev_id2context, dev_id2idx = create_mappings(data_dict['dev_uses_list'])\n",
    "        test_id2context, test_id2idx = create_mappings(data_dict['test_uses_list'])\n",
    "\n",
    "        # Merge train data\n",
    "        train_uses_merged = []\n",
    "        for row in data_dict['train_labels_list']:\n",
    "            identifier1_train = row['identifier1']\n",
    "            identifier2_train = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': train_id2context.get(identifier1_train),\n",
    "                'context2': train_id2context.get(identifier2_train),\n",
    "                'index_target_token1': train_id2idx.get(identifier1_train),\n",
    "                'index_target_token2': train_id2idx.get(identifier2_train),\n",
    "                'identifier1': identifier1_train,\n",
    "                'identifier2': identifier2_train,\n",
    "                'lemma': row['lemma'],\n",
    "                'mean_disagreement_cleaned': row['mean_disagreement_cleaned'],\n",
    "                'judgments': row['judgments'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            train_uses_merged.append(data_row)\n",
    "\n",
    "        # Merge dev data\n",
    "        dev_uses_merged = []\n",
    "        for row in data_dict['dev_labels_list']:\n",
    "            identifier1_dev = row['identifier1']\n",
    "            identifier2_dev = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': dev_id2context.get(identifier1_dev),\n",
    "                'context2': dev_id2context.get(identifier2_dev),\n",
    "                'index_target_token1': dev_id2idx.get(identifier1_dev),\n",
    "                'index_target_token2': dev_id2idx.get(identifier2_dev),\n",
    "                'identifier1': identifier1_dev,\n",
    "                'identifier2': identifier2_dev,\n",
    "                'lemma': row['lemma'],\n",
    "                'mean_disagreement_cleaned': row['mean_disagreement_cleaned'],\n",
    "                'judgments': row['judgments'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            dev_uses_merged.append(data_row)\n",
    "\n",
    "        test_uses_merged = []\n",
    "        for row in data_dict['test_labels_list']:\n",
    "            identifier1_test = row['identifier1']\n",
    "            identifier2_test = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': test_id2context.get(identifier1_test),\n",
    "                'context2': test_id2context.get(identifier2_test),\n",
    "                'index_target_token1': test_id2idx.get(identifier1_test),\n",
    "                'index_target_token2': test_id2idx.get(identifier2_test),\n",
    "                'identifier1': identifier1_test,\n",
    "                'identifier2': identifier2_test,\n",
    "                'lemma': row['lemma'],\n",
    "                'mean_disagreement_cleaned': row['mean_disagreement_cleaned'],\n",
    "                'judgments': row['judgments'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            test_uses_merged.append(data_row)\n",
    "\n",
    "        return pd.DataFrame(train_uses_merged), pd.DataFrame(dev_uses_merged), pd.DataFrame(test_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xk-q9egOpoEE",
    "outputId": "1c58f5f6-6015-4707-dfa5-51f74ceeed81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 15:13:50,165 - INFO - Found languages: ['chinese', 'spanish', 'german', 'norwegian', 'english', 'swedish', 'russian']\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoaders()\n",
    "data_loader.extract_zip_files()\n",
    "data_dict = data_loader.load_tsv_files()\n",
    "df_train_uses_merged, df_dev_uses_merged, df_test_uses_merged= data_loader.create_merged_dataframes(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XbvY4c3Rpqi8",
    "outputId": "4933a687-dfca-46f6-968f-23da87f7557e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>index_target_token1</th>\n",
       "      <th>index_target_token2</th>\n",
       "      <th>identifier1</th>\n",
       "      <th>identifier2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>mean_disagreement_cleaned</th>\n",
       "      <th>judgments</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>在大寨人自力更生建设山区革命精神的鼓舞下，他们下海围田，上山开荒，与自然奋战，向天公争粮</td>\n",
       "      <td>我一大早跑到了海边，沙滩上多了几条大木船，那是渔民下海夜里归来的吧</td>\n",
       "      <td>23:25</td>\n",
       "      <td>25:27</td>\n",
       "      <td>1966-14-12</td>\n",
       "      <td>1994-73-12</td>\n",
       "      <td>下海</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>”他瞅了瞅他现在穿的新皮大氅，又说：“过去我下海、在家，总是穿一件又腥又破的棉短袄；吃呢，一...</td>\n",
       "      <td>近年来，该报围绕“科技成果的评审和鉴定”、“上山与下海”、“科学研究如何选优和评价”等热点问...</td>\n",
       "      <td>22:24</td>\n",
       "      <td>25:27</td>\n",
       "      <td>1958-26-12</td>\n",
       "      <td>1994-53-12</td>\n",
       "      <td>下海</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>”他瞅了瞅他现在穿的新皮大氅，又说：“过去我下海、在家，总是穿一件又腥又破的棉短袄；吃呢，一...</td>\n",
       "      <td>我校乔刚同学在习作中给老师的信委婉地揭示出当前教师“下海”的现实，表达了学生对老师的依恋之情...</td>\n",
       "      <td>22:24</td>\n",
       "      <td>26:28</td>\n",
       "      <td>1958-26-12</td>\n",
       "      <td>1993-92-12</td>\n",
       "      <td>下海</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2, 1]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>驻守在沿海和海岛上的部队一面严密搜索海上敌情，保卫渔民生产，一面下海撒网捕捞鱼虾或在海边种海...</td>\n",
       "      <td>我们的老二，也就在这时候下定决心申请离职“下海”经商了</td>\n",
       "      <td>32:34</td>\n",
       "      <td>21:23</td>\n",
       "      <td>1961-42-12</td>\n",
       "      <td>1994-82-12</td>\n",
       "      <td>下海</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>驻守在沿海和海岛上的部队一面严密搜索海上敌情，保卫渔民生产，一面下海撒网捕捞鱼虾或在海边种海...</td>\n",
       "      <td>我一大早跑到了海边，沙滩上多了几条大木船，那是渔民下海夜里归来的吧</td>\n",
       "      <td>32:34</td>\n",
       "      <td>25:27</td>\n",
       "      <td>1961-42-12</td>\n",
       "      <td>1994-73-12</td>\n",
       "      <td>下海</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context1  \\\n",
       "0       在大寨人自力更生建设山区革命精神的鼓舞下，他们下海围田，上山开荒，与自然奋战，向天公争粮   \n",
       "1  ”他瞅了瞅他现在穿的新皮大氅，又说：“过去我下海、在家，总是穿一件又腥又破的棉短袄；吃呢，一...   \n",
       "2  ”他瞅了瞅他现在穿的新皮大氅，又说：“过去我下海、在家，总是穿一件又腥又破的棉短袄；吃呢，一...   \n",
       "3  驻守在沿海和海岛上的部队一面严密搜索海上敌情，保卫渔民生产，一面下海撒网捕捞鱼虾或在海边种海...   \n",
       "4  驻守在沿海和海岛上的部队一面严密搜索海上敌情，保卫渔民生产，一面下海撒网捕捞鱼虾或在海边种海...   \n",
       "\n",
       "                                            context2 index_target_token1  \\\n",
       "0                  我一大早跑到了海边，沙滩上多了几条大木船，那是渔民下海夜里归来的吧               23:25   \n",
       "1  近年来，该报围绕“科技成果的评审和鉴定”、“上山与下海”、“科学研究如何选优和评价”等热点问...               22:24   \n",
       "2  我校乔刚同学在习作中给老师的信委婉地揭示出当前教师“下海”的现实，表达了学生对老师的依恋之情...               22:24   \n",
       "3                        我们的老二，也就在这时候下定决心申请离职“下海”经商了               32:34   \n",
       "4                  我一大早跑到了海边，沙滩上多了几条大木船，那是渔民下海夜里归来的吧               32:34   \n",
       "\n",
       "  index_target_token2 identifier1 identifier2 lemma mean_disagreement_cleaned  \\\n",
       "0               25:27  1966-14-12  1994-73-12    下海                       0.0   \n",
       "1               25:27  1958-26-12  1994-53-12    下海                       0.0   \n",
       "2               26:28  1958-26-12  1993-92-12    下海                       1.0   \n",
       "3               21:23  1961-42-12  1994-82-12    下海                       0.0   \n",
       "4               25:27  1961-42-12  1994-73-12    下海                       0.0   \n",
       "\n",
       "  judgments language  \n",
       "0    [3, 3]  chinese  \n",
       "1    [2, 2]  chinese  \n",
       "2    [2, 1]  chinese  \n",
       "3    [2, 2]  chinese  \n",
       "4    [4, 4]  chinese  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>index_target_token1</th>\n",
       "      <th>index_target_token2</th>\n",
       "      <th>identifier1</th>\n",
       "      <th>identifier2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>mean_disagreement_cleaned</th>\n",
       "      <th>judgments</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>为把假货拒之门外，济南国有商店规范经营活动，把打假防假工作作为企业生存、发展的根本，落实责任...</td>\n",
       "      <td>在努力帮助“农民长期得实惠”的同时，唐山市也积极探索让“干部长期受教育”的机制</td>\n",
       "      <td>50:52</td>\n",
       "      <td>37:39</td>\n",
       "      <td>1996-98-26</td>\n",
       "      <td>2002-79-26</td>\n",
       "      <td>机制</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>例如，疗效需要进一步巩固和提高，方法有待于进一步丰富和发展，机制理论的研究有待于进一步加强，...</td>\n",
       "      <td>目前一方面在深入研究传染性肝炎发病机制，另一方面已把这些总结出来的经验推广到全市</td>\n",
       "      <td>30:32</td>\n",
       "      <td>17:19</td>\n",
       "      <td>1960-37-26</td>\n",
       "      <td>1960-44-26</td>\n",
       "      <td>机制</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[4, 2]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>其中《五十例阴虚患者症候分类分析及病理机制与治疗规律的探讨》、《对传染性肝炎的认识和探讨》等...</td>\n",
       "      <td>《条例》对公务员的录用、考核、升降、进出、监督、回避、轮换等都作出了明确的法令性的规定，有利...</td>\n",
       "      <td>19:21</td>\n",
       "      <td>61:63</td>\n",
       "      <td>1961-6-26</td>\n",
       "      <td>1993-65-26</td>\n",
       "      <td>机制</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[4, 2]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>为把假货拒之门外，济南国有商店规范经营活动，把打假防假工作作为企业生存、发展的根本，落实责任...</td>\n",
       "      <td>由于各地党政领导重视创安工作，从人防、物防、技防上进一步强化了社区治安防范机制，预防和控制犯...</td>\n",
       "      <td>50:52</td>\n",
       "      <td>37:39</td>\n",
       "      <td>1996-98-26</td>\n",
       "      <td>1997-93-26</td>\n",
       "      <td>机制</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>为把假货拒之门外，济南国有商店规范经营活动，把打假防假工作作为企业生存、发展的根本，落实责任...</td>\n",
       "      <td>温州机制活，民间资金丰富，国内外市场渠道畅通，在目前市场疲软、资金短缺的情况下应该发挥优势，...</td>\n",
       "      <td>50:52</td>\n",
       "      <td>2:4</td>\n",
       "      <td>1996-98-26</td>\n",
       "      <td>1998-88-26</td>\n",
       "      <td>机制</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context1  \\\n",
       "0  为把假货拒之门外，济南国有商店规范经营活动，把打假防假工作作为企业生存、发展的根本，落实责任...   \n",
       "1  例如，疗效需要进一步巩固和提高，方法有待于进一步丰富和发展，机制理论的研究有待于进一步加强，...   \n",
       "2  其中《五十例阴虚患者症候分类分析及病理机制与治疗规律的探讨》、《对传染性肝炎的认识和探讨》等...   \n",
       "3  为把假货拒之门外，济南国有商店规范经营活动，把打假防假工作作为企业生存、发展的根本，落实责任...   \n",
       "4  为把假货拒之门外，济南国有商店规范经营活动，把打假防假工作作为企业生存、发展的根本，落实责任...   \n",
       "\n",
       "                                            context2 index_target_token1  \\\n",
       "0            在努力帮助“农民长期得实惠”的同时，唐山市也积极探索让“干部长期受教育”的机制               50:52   \n",
       "1           目前一方面在深入研究传染性肝炎发病机制，另一方面已把这些总结出来的经验推广到全市               30:32   \n",
       "2  《条例》对公务员的录用、考核、升降、进出、监督、回避、轮换等都作出了明确的法令性的规定，有利...               19:21   \n",
       "3  由于各地党政领导重视创安工作，从人防、物防、技防上进一步强化了社区治安防范机制，预防和控制犯...               50:52   \n",
       "4  温州机制活，民间资金丰富，国内外市场渠道畅通，在目前市场疲软、资金短缺的情况下应该发挥优势，...               50:52   \n",
       "\n",
       "  index_target_token2 identifier1 identifier2 lemma mean_disagreement_cleaned  \\\n",
       "0               37:39  1996-98-26  2002-79-26    机制                       0.0   \n",
       "1               17:19  1960-37-26  1960-44-26    机制                       2.0   \n",
       "2               61:63   1961-6-26  1993-65-26    机制                       2.0   \n",
       "3               37:39  1996-98-26  1997-93-26    机制                       0.0   \n",
       "4                 2:4  1996-98-26  1998-88-26    机制                       0.0   \n",
       "\n",
       "  judgments language  \n",
       "0    [4, 4]  chinese  \n",
       "1    [4, 2]  chinese  \n",
       "2    [4, 2]  chinese  \n",
       "3    [4, 4]  chinese  \n",
       "4    [1, 1]  chinese  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>index_target_token1</th>\n",
       "      <th>index_target_token2</th>\n",
       "      <th>identifier1</th>\n",
       "      <th>identifier2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>mean_disagreement_cleaned</th>\n",
       "      <th>judgments</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...</td>\n",
       "      <td>在全国十二个中型氮肥厂中，生产能力提高最快，能源消耗最少，成本最低，利润最多，化肥包装质量最...</td>\n",
       "      <td>19:21</td>\n",
       "      <td>41:43</td>\n",
       "      <td>1964-16-30</td>\n",
       "      <td>1979-91-30</td>\n",
       "      <td>包装</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...</td>\n",
       "      <td>有些产品的质量、花色品种和包装装潢，与国际市场上同类产品相比，差距很大</td>\n",
       "      <td>19:21</td>\n",
       "      <td>13:15</td>\n",
       "      <td>1964-16-30</td>\n",
       "      <td>1978-5-30</td>\n",
       "      <td>包装</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...</td>\n",
       "      <td>如天津市调往外地的自行车，由于包装不好，每年报损二百四十多万元；北京市外调和市销的大瓶雪花膏...</td>\n",
       "      <td>19:21</td>\n",
       "      <td>15:17</td>\n",
       "      <td>1964-16-30</td>\n",
       "      <td>1978-40-30</td>\n",
       "      <td>包装</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[4, 3]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...</td>\n",
       "      <td>要认真搞好产品的包装，货运部门的职工要增强责任感，减少装卸、运输过程中的损失</td>\n",
       "      <td>19:21</td>\n",
       "      <td>8:10</td>\n",
       "      <td>1964-16-30</td>\n",
       "      <td>1978-4-30</td>\n",
       "      <td>包装</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...</td>\n",
       "      <td>人们记得，最近几年来，苏联外长葛罗米柯年复一年在联大提出的“重要而紧急”的议题，都是那些换了...</td>\n",
       "      <td>19:21</td>\n",
       "      <td>47:49</td>\n",
       "      <td>1964-16-30</td>\n",
       "      <td>1977-8-30</td>\n",
       "      <td>包装</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context1  \\\n",
       "0  在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...   \n",
       "1  在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...   \n",
       "2  在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...   \n",
       "3  在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...   \n",
       "4  在运动中，几百万商业职工在商品的收购、包装、运输、装卸、搬运、保管、销售等各个环节上，费尽心...   \n",
       "\n",
       "                                            context2 index_target_token1  \\\n",
       "0  在全国十二个中型氮肥厂中，生产能力提高最快，能源消耗最少，成本最低，利润最多，化肥包装质量最...               19:21   \n",
       "1                有些产品的质量、花色品种和包装装潢，与国际市场上同类产品相比，差距很大               19:21   \n",
       "2  如天津市调往外地的自行车，由于包装不好，每年报损二百四十多万元；北京市外调和市销的大瓶雪花膏...               19:21   \n",
       "3             要认真搞好产品的包装，货运部门的职工要增强责任感，减少装卸、运输过程中的损失               19:21   \n",
       "4  人们记得，最近几年来，苏联外长葛罗米柯年复一年在联大提出的“重要而紧急”的议题，都是那些换了...               19:21   \n",
       "\n",
       "  index_target_token2 identifier1 identifier2 lemma mean_disagreement_cleaned  \\\n",
       "0               41:43  1964-16-30  1979-91-30    包装                       0.0   \n",
       "1               13:15  1964-16-30   1978-5-30    包装                       0.0   \n",
       "2               15:17  1964-16-30  1978-40-30    包装                       1.0   \n",
       "3                8:10  1964-16-30   1978-4-30    包装                       0.0   \n",
       "4               47:49  1964-16-30   1977-8-30    包装                       1.0   \n",
       "\n",
       "  judgments language  \n",
       "0    [3, 3]  chinese  \n",
       "1    [3, 3]  chinese  \n",
       "2    [4, 3]  chinese  \n",
       "3    [3, 3]  chinese  \n",
       "4    [2, 3]  chinese  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train_uses_merged.head())\n",
    "display(df_dev_uses_merged.head())\n",
    "display(df_test_uses_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "BzxGW3t2puyG"
   },
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name=\"FacebookAI/xlm-roberta-base\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = XLMRobertaModel.from_pretrained(model_name)\n",
    "\n",
    "    def truncation_indices(self, target_subword_indices, truncation_tokens_before_target=0.5):\n",
    "        max_tokens = 512\n",
    "        n_target_subtokens = target_subword_indices.count(True)\n",
    "        tokens_before = int((max_tokens - n_target_subtokens) * truncation_tokens_before_target)\n",
    "        tokens_after = max_tokens - tokens_before - n_target_subtokens\n",
    "\n",
    "        lindex_target = target_subword_indices.index(True)\n",
    "        rindex_target = lindex_target + n_target_subtokens\n",
    "        lindex = max(lindex_target - tokens_before, 0)\n",
    "        rindex = rindex_target + tokens_after\n",
    "\n",
    "        return lindex, rindex\n",
    "\n",
    "    def get_target_token_embedding(self, context, index):\n",
    "        start_idx = int(str(index).strip().split(':')[0])\n",
    "        end_idx = int(str(index).strip().split(':')[1])\n",
    "\n",
    "        inputs = self.tokenizer(context, return_tensors=\"pt\",\n",
    "                              return_offsets_mapping=True, add_special_tokens=False)\n",
    "\n",
    "        offset_mapping = inputs['offset_mapping'][0].tolist()\n",
    "        input_ids = inputs['input_ids']\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "        subwords_bool_mask = [\n",
    "            (start <= start_idx < end) or (start < end_idx <= end)\n",
    "            or (start_idx <= start and end <= end_idx)\n",
    "            for start, end in offset_mapping\n",
    "        ]\n",
    "\n",
    "        if len(input_ids[0]) > 512:\n",
    "            lindex, rindex = self.truncation_indices(subwords_bool_mask)\n",
    "            tokens = tokens[lindex:rindex]\n",
    "            input_ids = input_ids[:, lindex:rindex]\n",
    "            subwords_bool_mask = subwords_bool_mask[lindex:rindex]\n",
    "            inputs['input_ids'] = input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs['input_ids'])\n",
    "\n",
    "        target_embeddings = outputs.last_hidden_state[0][subwords_bool_mask]\n",
    "        return target_embeddings.mean(dim=0).numpy()\n",
    "\n",
    "    def generate_embeddings(self, df, file_name):\n",
    "        id2embedding = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            identifier1 = row['identifier1']\n",
    "            identifier2 = row['identifier2']\n",
    "\n",
    "            if identifier1 not in id2embedding:\n",
    "                embedding1 = self.get_target_token_embedding(row['context1'],\n",
    "                                                          row['index_target_token1'])\n",
    "                id2embedding[identifier1] = embedding1\n",
    "\n",
    "            if identifier2 not in id2embedding:\n",
    "                embedding2 = self.get_target_token_embedding(row['context2'],\n",
    "                                                          row['index_target_token2'])\n",
    "                id2embedding[identifier2] = embedding2\n",
    "\n",
    "        np.savez(file_name, **id2embedding)\n",
    "        return id2embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2yAGAH6WqAKo"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "#embedding_generator = EmbeddingGenerator()\n",
    "#train_embeddings = embedding_generator.generate_embeddings(df_train_uses_merged,\n",
    "                                                         #'subtask2_train_embeddings.npz')\n",
    "\n",
    "#dev_embeddings = embedding_generator.generate_embeddings(df_dev_uses_merged,\n",
    "                                                       #'subtask2_dev_embeddings.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "a4YrCSXvYsb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "roberta_model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, embeddings, max_len=512, is_test=False):\n",
    "        self.data = data\n",
    "        self.embeddings = self.load_embeddings(embeddings)\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_embeddings(self, file_name):\n",
    "        \"\"\"Load embeddings from a single .npz file.\"\"\"\n",
    "        try:\n",
    "            loaded_embeddings = np.load(file_name)\n",
    "            embeddings = {identifier: loaded_embeddings[identifier]\n",
    "                        for identifier in loaded_embeddings.files}\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings from {file_name}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        identifier1 = row['identifier1']\n",
    "        identifier2 = row['identifier2']\n",
    "\n",
    "        # Get embeddings with zero padding if not found\n",
    "        embedding1 = self.embeddings.get(identifier1, np.zeros(self.max_len))\n",
    "        embedding2 = self.embeddings.get(identifier2, np.zeros(self.max_len))\n",
    "\n",
    "        # Convert embeddings to tensors\n",
    "        target_embedding1 = torch.tensor(embedding1, dtype=torch.float)\n",
    "        target_embedding2 = torch.tensor(embedding2, dtype=torch.float)\n",
    "\n",
    "        if not self.is_test:\n",
    "            # Get the median_cleaned value as a float for regression\n",
    "            target = torch.tensor(float(row['mean_disagreement_cleaned']), dtype=torch.float)\n",
    "\n",
    "            return {\n",
    "                'target_embedding1': target_embedding1,\n",
    "                'target_embedding2': target_embedding2,\n",
    "                'target': target\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'target_embedding1': target_embedding1,\n",
    "                'target_embedding2': target_embedding2\n",
    "            }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle dictionary outputs.\"\"\"\n",
    "    return {key: default_collate([d[key] for d in batch]) for key in batch[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel as PretrainedXLMRobertaModel\n",
    "\n",
    "class XLMRobertaModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, hidden_dims=512, dropout_rate=0.2, output_dim=1):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.roberta = PretrainedXLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "        \n",
    "        input_size = embedding_dim * 2\n",
    "\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(input_size, output_dim)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embedding1, embedding2):\n",
    "        # Validate input shapes\n",
    "        assert embedding1.size(0) == embedding2.size(0), \"Batch sizes must match\"\n",
    "        assert embedding1.size(1) == embedding2.size(1), \"Embeddings must have the same dimension\"\n",
    "\n",
    "        # Concatenate embeddings\n",
    "        features = torch.cat((embedding1, embedding2), dim=1)\n",
    "        return self.regression(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self,\n",
    "                 output_dim=1,\n",
    "                 embedding_dim=768,\n",
    "                 hidden_dims=[512],\n",
    "                 learning_rate=1e-4,\n",
    "                 batch_size=32,\n",
    "                 num_epochs=10,\n",
    "                 dropout_rate=0.2,\n",
    "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "\n",
    "        self.model = XLMRobertaModel(\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout_rate=dropout_rate,\n",
    "            output_dim=output_dim\n",
    "        ).to(device)\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = device\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        # Add learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "        )\n",
    "\n",
    "    def train(self, train_dataset, val_dataset=None):\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True if self.device == 'cuda' else False\n",
    "        )\n",
    "\n",
    "        val_loader = None\n",
    "        if val_dataset:\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                pin_memory=True if self.device == 'cuda' else False\n",
    "            )\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            train_batches = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')\n",
    "\n",
    "            for batch in train_batches:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                embedding1 = batch['target_embedding1'].to(self.device)\n",
    "                embedding2 = batch['target_embedding2'].to(self.device)\n",
    "\n",
    "                # Targets kept as a 1D tensor\n",
    "                targets = batch['target'].to(self.device).float()\n",
    "                assert embedding1.size(0) == embedding2.size(0) == targets.size(0), \"Batch sizes must match\"\n",
    "                \n",
    "                outputs = self.model(embedding1, embedding2)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                loss.backward()\n",
    "                # Add gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                train_batches.set_postfix({'train_loss': loss.item()})\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "\n",
    "            if val_loader:\n",
    "                val_loss = self.evaluate(val_loader)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "                print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {val_loss:.4f}')\n",
    "\n",
    "                # Update learning rate scheduler\n",
    "                self.scheduler.step(val_loss)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'val_loss': val_loss,\n",
    "                    }, 'xlm_model.pt')\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}')\n",
    "\n",
    "        return self.train_losses, self.val_losses\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                embedding1 = batch['target_embedding1'].to(self.device)\n",
    "                embedding2 = batch['target_embedding2'].to(self.device)\n",
    "                targets = batch['target'].to(self.device).float()\n",
    "\n",
    "                outputs = self.model(embedding1, embedding2)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        return total_val_loss / len(val_loader)\n",
    "\n",
    "    def predict(self, test_dataset):\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                embedding1 = batch['target_embedding1'].to(self.device)\n",
    "                embedding2 = batch['target_embedding2'].to(self.device)\n",
    "\n",
    "                outputs = self.model(embedding1, embedding2)\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data=df_train_uses_merged,\n",
    "                            embeddings='subtask2_train_embeddings.npz',\n",
    "                            max_len=512,\n",
    "                            is_test=False)\n",
    "\n",
    "dev_dataset = CustomDataset(data=df_dev_uses_merged,\n",
    "                        embeddings='subtask2_dev_embeddings.npz',\n",
    "                        max_len=512,\n",
    "                        is_test=False)\n",
    "test_dataset = CustomDataset(data=df_test_uses_merged,\n",
    "                        embeddings='subtask2_test_embeddings.npz',\n",
    "                        max_len=512,\n",
    "                        is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "Epoch 1/10:   0%|          | 0/2569 [00:00<?, ?it/s]/home/sarumio/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/10:  99%|█████████▉| 2554/2569 [00:11<00:00, 246.92it/s, train_loss=0.55] /home/sarumio/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/10: 100%|██████████| 2569/2569 [00:11<00:00, 214.33it/s, train_loss=0.099]\n",
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.5526, Val Loss = 0.4911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2569/2569 [00:13<00:00, 195.33it/s, train_loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.5473, Val Loss = 0.4889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 2569/2569 [00:13<00:00, 186.03it/s, train_loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.5467, Val Loss = 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 2569/2569 [00:13<00:00, 189.15it/s, train_loss=0.127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.5464, Val Loss = 0.4951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 2569/2569 [00:12<00:00, 203.60it/s, train_loss=1.97] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.5468, Val Loss = 0.4897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 2569/2569 [00:12<00:00, 200.59it/s, train_loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.5463, Val Loss = 0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 2569/2569 [00:12<00:00, 200.60it/s, train_loss=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.5461, Val Loss = 0.4881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 2569/2569 [00:14<00:00, 179.12it/s, train_loss=1.89] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.5468, Val Loss = 0.4932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 2569/2569 [00:13<00:00, 192.09it/s, train_loss=0.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.5461, Val Loss = 0.4875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 2569/2569 [00:13<00:00, 192.62it/s, train_loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.5457, Val Loss = 0.4837\n"
     ]
    }
   ],
   "source": [
    "learner = Learner(\n",
    "    embedding_dim=768,  # XLM-RoBERTa base embedding dimension\n",
    "    hidden_dims=512,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=32,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "train_losses, val_losses = learner.train(train_dataset, dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpRWNYdEWToV",
    "outputId": "f780e7c0-674a-4365-dde5-63e758752904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6151509 ],\n",
       "       [0.61847496],\n",
       "       [0.5925765 ],\n",
       "       ...,\n",
       "       [0.5720765 ],\n",
       "       [0.5463245 ],\n",
       "       [0.56814146]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = learner.predict(test_dataset)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_Ovy2GRFevMP"
   },
   "outputs": [],
   "source": [
    "df_test_uses_merged['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "jXlNBynI9cro"
   },
   "outputs": [],
   "source": [
    "out_dir = 'answer/'\n",
    "if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "answer_df = df_test_uses_merged[['identifier1', 'identifier2', 'prediction', 'language']]\n",
    "answer_df = answer_df.reset_index(drop= True)\n",
    "for i in list(answer_df[\"language\"].value_counts().index):\n",
    "    df_temp = answer_df[answer_df[\"language\"]==i]\n",
    "    df_temp = df_temp.drop('language', axis=1)\n",
    "    df_temp.to_csv('answer/' +i +'.tsv',index = False, sep='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "\n",
    "with ZipFile('answer.zip', 'w') as zipf:\n",
    "    for root, _, files in os.walk(out_dir):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), arcname=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved truth labels for chinese at refi/chinese/labels.tsv\n",
      "Saved truth labels for spanish at refi/spanish/labels.tsv\n",
      "Saved truth labels for german at refi/german/labels.tsv\n",
      "Saved truth labels for norwegian at refi/norwegian/labels.tsv\n",
      "Saved truth labels for english at refi/english/labels.tsv\n",
      "Saved truth labels for swedish at refi/swedish/labels.tsv\n",
      "Saved truth labels for russian at refi/russian/labels.tsv\n"
     ]
    }
   ],
   "source": [
    "def true_labels(df, ref):\n",
    "\n",
    "    if not os.path.exists(ref):\n",
    "        os.makedirs(ref)\n",
    "\n",
    "    languages = df['language'].unique()\n",
    "\n",
    "    for language in languages:\n",
    "        lang_df = df[df['language'] == language][['identifier1', 'identifier2', 'mean_disagreement_cleaned']]\n",
    "\n",
    "        lang_dir = os.path.join(ref, language)\n",
    "        if not os.path.exists(lang_dir):\n",
    "            os.makedirs(lang_dir)\n",
    "\n",
    "        lang_file_path = os.path.join(lang_dir, 'labels.tsv')\n",
    "        lang_df.to_csv(lang_file_path, sep='\\t', index=False, quoting=csv.QUOTE_NONE)\n",
    "        print(f\"Saved truth labels for {language} at {lang_file_path}\")\n",
    "\n",
    "true_labels(df_test_uses_merged, 'refi')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
