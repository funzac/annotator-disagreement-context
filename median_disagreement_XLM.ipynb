{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9X-yBKUtgcNZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from scipy.optimize import minimize\n",
    "from transformers import AutoModel, AutoTokenizer, XLMRobertaModel\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v0bFh7Hfx8bW"
   },
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self):\n",
    "        self.setup_paths()\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def setup_paths(self):\n",
    "        self.path_dev = 'dev/'\n",
    "        self.path_train = 'train/'\n",
    "        self.path_test = 'test/'\n",
    "        self.path_output = 'answer/'\n",
    "        self.path_testh = 'testh/'\n",
    "\n",
    "        for path in [self.path_dev, self.path_train, self.path_test, self.path_output, self.path_testh]:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "    def extract_zip_files(self):\n",
    "        # Extract dev.zip\n",
    "        if not os.listdir(self.path_dev):\n",
    "            with ZipFile('dev.zip', 'r') as dev:\n",
    "                dev.extractall(self.path_dev)\n",
    "\n",
    "        # Extract train.zip\n",
    "        if not os.listdir(self.path_train):\n",
    "            with ZipFile('train.zip', 'r') as train:\n",
    "                train.extractall(self.path_train)\n",
    "\n",
    "        # Extract test.zip\n",
    "        if not os.listdir(self.path_test):\n",
    "            with ZipFile('test.zip', 'r') as test:\n",
    "                test.extractall(self.path_test)\n",
    "\n",
    "        # Extract testh.zip\n",
    "        if not os.listdir(self.path_testh):\n",
    "            with ZipFile('test_hidden1.zip', 'r') as testh:\n",
    "                testh.extractall(self.path_testh)\n",
    "\n",
    "    def load_tsv_files(self):\n",
    "        languages = os.listdir(self.path_train)\n",
    "        self.logger.info(f\"Found languages: {languages}\")\n",
    "\n",
    "        # Initialize file paths\n",
    "        label_file_paths_train = []\n",
    "        uses_file_paths_train = []\n",
    "        label_file_paths_dev = []\n",
    "        uses_file_paths_dev = []\n",
    "        instance_file_paths_test = []\n",
    "        uses_file_paths_test = []\n",
    "        label_file_paths_test = []\n",
    "\n",
    "\n",
    "        for lang in languages:\n",
    "            label_file_paths_train.append(f\"{self.path_train}{lang}/labels.tsv\")\n",
    "            uses_file_paths_train.append(f\"{self.path_train}{lang}/uses.tsv\")\n",
    "            label_file_paths_dev.append(f\"{self.path_dev}{lang}/labels.tsv\")\n",
    "            uses_file_paths_dev.append(f\"{self.path_dev}{lang}/uses.tsv\")\n",
    "            instance_file_paths_test.append(f\"{self.path_test}{lang}/instances.tsv\")\n",
    "            uses_file_paths_test.append(f\"{self.path_test}{lang}/uses.tsv\")\n",
    "            label_file_paths_test.append(f\"{self.path_testh}{lang}/labels.tsv\")\n",
    "        paths = {\n",
    "            'train_labels_list': label_file_paths_train,\n",
    "            'train_uses_list': uses_file_paths_train,\n",
    "            'dev_labels_list': label_file_paths_dev,\n",
    "            'dev_uses_list': uses_file_paths_dev,\n",
    "            'test_uses_list': uses_file_paths_test,\n",
    "            'test_instances_list': instance_file_paths_test,\n",
    "            'test_labels_list': label_file_paths_test\n",
    "        }\n",
    "\n",
    "        data_dict = {key: [] for key in paths.keys()}\n",
    "\n",
    "        for save_path, path_list in paths.items():\n",
    "            for path in path_list:\n",
    "                with open(path, encoding='utf-8') as tsvfile:\n",
    "                    language = path.split('/')[1]\n",
    "                    reader = csv.DictReader(tsvfile, delimiter='\\t',\n",
    "                                         quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "                    for row in reader:\n",
    "                        row['language'] = language\n",
    "                        data_dict[save_path].append(row)\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    def create_merged_dataframes(self, data_dict):\n",
    "        def create_mappings(uses_list):\n",
    "            id2context = {}\n",
    "            id2idx = {}\n",
    "            for row in uses_list:\n",
    "                identifier = row['identifier']\n",
    "                id2context[identifier] = row['context']\n",
    "                id2idx[identifier] = row['indices_target_token']\n",
    "            return id2context, id2idx\n",
    "\n",
    "        # Create mappings\n",
    "        train_id2context, train_id2idx = create_mappings(data_dict['train_uses_list'])\n",
    "        dev_id2context, dev_id2idx = create_mappings(data_dict['dev_uses_list'])\n",
    "        test_id2context, test_id2idx = create_mappings(data_dict['test_uses_list'])\n",
    "\n",
    "        # Merge train data\n",
    "        train_uses_merged = []\n",
    "        for row in data_dict['train_labels_list']:\n",
    "            identifier1_train = row['identifier1']\n",
    "            identifier2_train = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': train_id2context.get(identifier1_train),\n",
    "                'context2': train_id2context.get(identifier2_train),\n",
    "                'index_target_token1': train_id2idx.get(identifier1_train),\n",
    "                'index_target_token2': train_id2idx.get(identifier2_train),\n",
    "                'identifier1': identifier1_train,\n",
    "                'identifier2': identifier2_train,\n",
    "                'lemma': row['lemma'],\n",
    "                'median_cleaned': row['median_cleaned'],\n",
    "                'judgments': row['judgments'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            train_uses_merged.append(data_row)\n",
    "\n",
    "\n",
    "        dev_uses_merged = []\n",
    "        for row in data_dict['dev_labels_list']:\n",
    "            identifier1_dev = row['identifier1']\n",
    "            identifier2_dev = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': dev_id2context.get(identifier1_dev),\n",
    "                'context2': dev_id2context.get(identifier2_dev),\n",
    "                'index_target_token1': dev_id2idx.get(identifier1_dev),\n",
    "                'index_target_token2': dev_id2idx.get(identifier2_dev),\n",
    "                'identifier1': identifier1_dev,\n",
    "                'identifier2': identifier2_dev,\n",
    "                'lemma': row['lemma'],\n",
    "                'median_cleaned': row['median_cleaned'],\n",
    "                'judgments': row['judgments'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            dev_uses_merged.append(data_row)\n",
    "\n",
    "        # Merge test data\n",
    "        test_uses_merged = []\n",
    "        for row in data_dict['test_labels_list']:\n",
    "            identifier1_test = row['identifier1']\n",
    "            identifier2_test = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': test_id2context.get(identifier1_test),\n",
    "                'context2': test_id2context.get(identifier2_test),\n",
    "                'index_target_token1': test_id2idx.get(identifier1_test),\n",
    "                'index_target_token2': test_id2idx.get(identifier2_test),\n",
    "                'identifier1': identifier1_test,\n",
    "                'identifier2': identifier2_test,\n",
    "                'lemma': row['lemma'],\n",
    "                'median_cleaned': row['median_cleaned'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            test_uses_merged.append(data_row)\n",
    "\n",
    "        return pd.DataFrame(train_uses_merged), pd.DataFrame(dev_uses_merged), pd.DataFrame(test_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "-kTxkcNjglr4",
    "outputId": "c2820d39-2400-416b-b1f0-5a1c0e860cd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 12:34:55,721 - INFO - Found languages: ['norwegian', 'german', 'chinese', 'spanish', 'english', 'russian', 'swedish']\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoaders()\n",
    "data_loader.extract_zip_files()\n",
    "data_dict = data_loader.load_tsv_files()\n",
    "df_train_uses_merged, df_dev_uses_merged, df_test_uses_merged = data_loader.create_merged_dataframes(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "V6Myj4bQgrD2",
    "outputId": "273cd893-9d2b-4f7d-972e-1ab6b4935b09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "median_cleaned\n",
       "4.0    30257\n",
       "1.0     7099\n",
       "3.0     5967\n",
       "2.0     4510\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_uses_merged['median_cleaned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YYnRujG9x8bb",
    "outputId": "05022bf0-6bbe-4b4e-a801-5107d730d365"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>index_target_token1</th>\n",
       "      <th>index_target_token2</th>\n",
       "      <th>identifier1</th>\n",
       "      <th>identifier2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>median_cleaned</th>\n",
       "      <th>judgments</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>573 F LR N (Samisk) (Sang- (Utkast Salmer boke...</td>\n",
       "      <td>Det er saerlig knyttet anfektelser til spørsmå...</td>\n",
       "      <td>116:126</td>\n",
       "      <td>23:34</td>\n",
       "      <td>1970-2015_anfektelse_0</td>\n",
       "      <td>1970-2015_anfektelse_15</td>\n",
       "      <td>anfektelse</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trøst dem i all nød og anfektelse med din Hell...</td>\n",
       "      <td>Menneskelivet og kristenlivet I — V Kveldsbønn...</td>\n",
       "      <td>23:33</td>\n",
       "      <td>79:89</td>\n",
       "      <td>1970-2015_anfektelse_16</td>\n",
       "      <td>1970-2015_anfektelse_2</td>\n",
       "      <td>anfektelse</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trøst dem i all nød og anfektelse med din Hell...</td>\n",
       "      <td>Trøst dem i all nød og anfektelse med din Hell...</td>\n",
       "      <td>23:33</td>\n",
       "      <td>23:33</td>\n",
       "      <td>1970-2015_anfektelse_16</td>\n",
       "      <td>1970-2015_anfektelse_17</td>\n",
       "      <td>anfektelse</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Det er saerlig knyttet anfektelser til spørsmå...</td>\n",
       "      <td>Dernest hadde Løvenskiold anfektelser på grunn...</td>\n",
       "      <td>23:34</td>\n",
       "      <td>26:37</td>\n",
       "      <td>1970-2015_anfektelse_15</td>\n",
       "      <td>1970-2015_anfektelse_5</td>\n",
       "      <td>anfektelse</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Og selv ikke kommunene later til å ha noen sae...</td>\n",
       "      <td>Dernest hadde Løvenskiold anfektelser på grunn...</td>\n",
       "      <td>52:63</td>\n",
       "      <td>26:37</td>\n",
       "      <td>1970-2015_anfektelse_11</td>\n",
       "      <td>1970-2015_anfektelse_5</td>\n",
       "      <td>anfektelse</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context1  \\\n",
       "0  573 F LR N (Samisk) (Sang- (Utkast Salmer boke...   \n",
       "1  Trøst dem i all nød og anfektelse med din Hell...   \n",
       "2  Trøst dem i all nød og anfektelse med din Hell...   \n",
       "3  Det er saerlig knyttet anfektelser til spørsmå...   \n",
       "4  Og selv ikke kommunene later til å ha noen sae...   \n",
       "\n",
       "                                            context2 index_target_token1  \\\n",
       "0  Det er saerlig knyttet anfektelser til spørsmå...             116:126   \n",
       "1  Menneskelivet og kristenlivet I — V Kveldsbønn...               23:33   \n",
       "2  Trøst dem i all nød og anfektelse med din Hell...               23:33   \n",
       "3  Dernest hadde Løvenskiold anfektelser på grunn...               23:34   \n",
       "4  Dernest hadde Løvenskiold anfektelser på grunn...               52:63   \n",
       "\n",
       "  index_target_token2              identifier1              identifier2  \\\n",
       "0               23:34   1970-2015_anfektelse_0  1970-2015_anfektelse_15   \n",
       "1               79:89  1970-2015_anfektelse_16   1970-2015_anfektelse_2   \n",
       "2               23:33  1970-2015_anfektelse_16  1970-2015_anfektelse_17   \n",
       "3               26:37  1970-2015_anfektelse_15   1970-2015_anfektelse_5   \n",
       "4               26:37  1970-2015_anfektelse_11   1970-2015_anfektelse_5   \n",
       "\n",
       "        lemma median_cleaned  judgments   language  \n",
       "0  anfektelse            3.0     [3, 3]  norwegian  \n",
       "1  anfektelse            4.0     [4, 4]  norwegian  \n",
       "2  anfektelse            4.0  [4, 4, 4]  norwegian  \n",
       "3  anfektelse            4.0  [4, 4, 4]  norwegian  \n",
       "4  anfektelse            4.0  [4, 4, 4]  norwegian  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>index_target_token1</th>\n",
       "      <th>index_target_token2</th>\n",
       "      <th>identifier1</th>\n",
       "      <th>identifier2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>median_cleaned</th>\n",
       "      <th>judgments</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>av forordningen, kan gi hjelp til at det blir ...</td>\n",
       "      <td>Finner retten på noe tidspunkt at etterforskni...</td>\n",
       "      <td>90:104</td>\n",
       "      <td>34:50</td>\n",
       "      <td>1929-1965_etterforskning_9</td>\n",
       "      <td>1970-2015_etterforskning_1824</td>\n",
       "      <td>etterforskning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Og det sier seg selv at det da ikke alltid er ...</td>\n",
       "      <td>Dette førte til ny interesse for saken i offen...</td>\n",
       "      <td>112:126</td>\n",
       "      <td>298:312</td>\n",
       "      <td>1929-1965_etterforskning_54</td>\n",
       "      <td>1970-2015_etterforskning_1032</td>\n",
       "      <td>etterforskning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Resultatet av drøftelsene på Stortinget i 1948...</td>\n",
       "      <td>Det følgende vil redegjøre for enkelte øvrige ...</td>\n",
       "      <td>215:229</td>\n",
       "      <td>334:350</td>\n",
       "      <td>1929-1965_etterforskning_52</td>\n",
       "      <td>1970-2015_etterforskning_998</td>\n",
       "      <td>etterforskning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Resultatet av drøftelsene på Stortinget i 1948...</td>\n",
       "      <td>Alene den lange tid - 25 år - som har gått sid...</td>\n",
       "      <td>215:229</td>\n",
       "      <td>180:194</td>\n",
       "      <td>1929-1965_etterforskning_52</td>\n",
       "      <td>1970-2015_etterforskning_557</td>\n",
       "      <td>etterforskning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anm.: 1 anledning spørsmålet om etterforskning...</td>\n",
       "      <td>Målet må vaere å øke ressursene ved å foreta e...</td>\n",
       "      <td>32:46</td>\n",
       "      <td>79:93</td>\n",
       "      <td>1929-1965_etterforskning_62</td>\n",
       "      <td>1970-2015_etterforskning_318</td>\n",
       "      <td>etterforskning</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context1  \\\n",
       "0  av forordningen, kan gi hjelp til at det blir ...   \n",
       "1  Og det sier seg selv at det da ikke alltid er ...   \n",
       "2  Resultatet av drøftelsene på Stortinget i 1948...   \n",
       "3  Resultatet av drøftelsene på Stortinget i 1948...   \n",
       "4  Anm.: 1 anledning spørsmålet om etterforskning...   \n",
       "\n",
       "                                            context2 index_target_token1  \\\n",
       "0  Finner retten på noe tidspunkt at etterforskni...              90:104   \n",
       "1  Dette førte til ny interesse for saken i offen...             112:126   \n",
       "2  Det følgende vil redegjøre for enkelte øvrige ...             215:229   \n",
       "3  Alene den lange tid - 25 år - som har gått sid...             215:229   \n",
       "4  Målet må vaere å øke ressursene ved å foreta e...               32:46   \n",
       "\n",
       "  index_target_token2                  identifier1  \\\n",
       "0               34:50   1929-1965_etterforskning_9   \n",
       "1             298:312  1929-1965_etterforskning_54   \n",
       "2             334:350  1929-1965_etterforskning_52   \n",
       "3             180:194  1929-1965_etterforskning_52   \n",
       "4               79:93  1929-1965_etterforskning_62   \n",
       "\n",
       "                     identifier2           lemma median_cleaned  judgments  \\\n",
       "0  1970-2015_etterforskning_1824  etterforskning            4.0  [4, 4, 4]   \n",
       "1  1970-2015_etterforskning_1032  etterforskning            4.0     [4, 4]   \n",
       "2   1970-2015_etterforskning_998  etterforskning            4.0     [4, 4]   \n",
       "3   1970-2015_etterforskning_557  etterforskning            4.0     [4, 4]   \n",
       "4   1970-2015_etterforskning_318  etterforskning            4.0     [4, 4]   \n",
       "\n",
       "    language  \n",
       "0  norwegian  \n",
       "1  norwegian  \n",
       "2  norwegian  \n",
       "3  norwegian  \n",
       "4  norwegian  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>index_target_token1</th>\n",
       "      <th>index_target_token2</th>\n",
       "      <th>identifier1</th>\n",
       "      <th>identifier2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>median_cleaned</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3. Egne hjem for krigspensjonister, der graden...</td>\n",
       "      <td>Han gjennomførte flere førstebestigninger både...</td>\n",
       "      <td>249:254</td>\n",
       "      <td>695:700</td>\n",
       "      <td>1980-1990_syden_4</td>\n",
       "      <td>2012-2019_syden_4</td>\n",
       "      <td>Syden</td>\n",
       "      <td>4.0</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>«Hvi kan I innbille Jer, I, dårlige menneske, ...</td>\n",
       "      <td>For kortere perioder kan det vaere aktuelt å l...</td>\n",
       "      <td>128:133</td>\n",
       "      <td>211:216</td>\n",
       "      <td>1980-1990_syden_7</td>\n",
       "      <td>1980-1990_syden_8</td>\n",
       "      <td>Syden</td>\n",
       "      <td>4.0</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«Hvi kan I innbille Jer, I, dårlige menneske, ...</td>\n",
       "      <td>utdannelse Hovedfag i samfunnsgeografi ved Uni...</td>\n",
       "      <td>128:133</td>\n",
       "      <td>164:169</td>\n",
       "      <td>1980-1990_syden_7</td>\n",
       "      <td>2012-2019_syden_111</td>\n",
       "      <td>Syden</td>\n",
       "      <td>4.0</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>«Hvi kan I innbille Jer, I, dårlige menneske, ...</td>\n",
       "      <td>Men da artisjokk helkokt i klarnet smør skulle...</td>\n",
       "      <td>128:133</td>\n",
       "      <td>82:87</td>\n",
       "      <td>1980-1990_syden_7</td>\n",
       "      <td>2012-2019_syden_187</td>\n",
       "      <td>Syden</td>\n",
       "      <td>4.0</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>«Hvi kan I innbille Jer, I, dårlige menneske, ...</td>\n",
       "      <td>Pris pr. person i 2-romsleilighet på Sea Melod...</td>\n",
       "      <td>128:133</td>\n",
       "      <td>316:321</td>\n",
       "      <td>1980-1990_syden_7</td>\n",
       "      <td>2012-2019_syden_22</td>\n",
       "      <td>Syden</td>\n",
       "      <td>4.0</td>\n",
       "      <td>norwegian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context1  \\\n",
       "0  3. Egne hjem for krigspensjonister, der graden...   \n",
       "1  «Hvi kan I innbille Jer, I, dårlige menneske, ...   \n",
       "2  «Hvi kan I innbille Jer, I, dårlige menneske, ...   \n",
       "3  «Hvi kan I innbille Jer, I, dårlige menneske, ...   \n",
       "4  «Hvi kan I innbille Jer, I, dårlige menneske, ...   \n",
       "\n",
       "                                            context2 index_target_token1  \\\n",
       "0  Han gjennomførte flere førstebestigninger både...             249:254   \n",
       "1  For kortere perioder kan det vaere aktuelt å l...             128:133   \n",
       "2  utdannelse Hovedfag i samfunnsgeografi ved Uni...             128:133   \n",
       "3  Men da artisjokk helkokt i klarnet smør skulle...             128:133   \n",
       "4  Pris pr. person i 2-romsleilighet på Sea Melod...             128:133   \n",
       "\n",
       "  index_target_token2        identifier1          identifier2  lemma  \\\n",
       "0             695:700  1980-1990_syden_4    2012-2019_syden_4  Syden   \n",
       "1             211:216  1980-1990_syden_7    1980-1990_syden_8  Syden   \n",
       "2             164:169  1980-1990_syden_7  2012-2019_syden_111  Syden   \n",
       "3               82:87  1980-1990_syden_7  2012-2019_syden_187  Syden   \n",
       "4             316:321  1980-1990_syden_7   2012-2019_syden_22  Syden   \n",
       "\n",
       "  median_cleaned   language  \n",
       "0            4.0  norwegian  \n",
       "1            4.0  norwegian  \n",
       "2            4.0  norwegian  \n",
       "3            4.0  norwegian  \n",
       "4            4.0  norwegian  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train_uses_merged.head())\n",
    "display(df_dev_uses_merged.head())\n",
    "display(df_test_uses_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c7kjVavgx8bc",
    "outputId": "d66a162a-80cf-4a9c-d033-378de71490b4"
   },
   "outputs": [],
   "source": [
    "#df_dev_uses_merged['median_cleaned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bSBl2Ry-x8bd",
    "outputId": "3ba32537-4f0b-45cd-d73a-5f7971dbf269"
   },
   "outputs": [],
   "source": [
    "#df_train_uses_merged['median_cleaned'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_uses_merged['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>index_target_token1</th>\n",
       "      <th>index_target_token2</th>\n",
       "      <th>identifier1</th>\n",
       "      <th>identifier2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>median_cleaned</th>\n",
       "      <th>judgments</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28437</th>\n",
       "      <td>I'd landed in early afternoon, local time.</td>\n",
       "      <td>Old shopping lists and ticket stubs and wads o...</td>\n",
       "      <td>20:29</td>\n",
       "      <td>104:113</td>\n",
       "      <td>fic_1987_780057.txt-318-5</td>\n",
       "      <td>fic_1992_40272.txt-627-18</td>\n",
       "      <td>afternoon_nn</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>I'd landed in early afternoon, local time.</td>\n",
       "      <td>@@774723 1 Two Years Earlier // One weekday af...</td>\n",
       "      <td>20:29</td>\n",
       "      <td>44:53</td>\n",
       "      <td>fic_1987_780057.txt-318-5</td>\n",
       "      <td>nf_2008_774723.txt-0-8</td>\n",
       "      <td>afternoon_nn</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28439</th>\n",
       "      <td>I'd landed in early afternoon, local time.</td>\n",
       "      <td>afternoon that the Indian bought me, he starte...</td>\n",
       "      <td>20:29</td>\n",
       "      <td>0:9</td>\n",
       "      <td>fic_1987_780057.txt-318-5</td>\n",
       "      <td>nf_1849_762578.txt-1803-0</td>\n",
       "      <td>afternoon_nn</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28440</th>\n",
       "      <td>Mrs. Daggett, nettled by her sister's hint, ro...</td>\n",
       "      <td>She remained in company with the cows all the ...</td>\n",
       "      <td>91:100</td>\n",
       "      <td>46:55</td>\n",
       "      <td>fic_1822_7275.txt-216-23</td>\n",
       "      <td>fic_1850_2310.txt-1321-9</td>\n",
       "      <td>afternoon_nn</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4, 4]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28441</th>\n",
       "      <td>Elvira had just got possession, by stealth, of...</td>\n",
       "      <td>On those afternoons by the sea, we knitted, Sh...</td>\n",
       "      <td>201:210</td>\n",
       "      <td>9:19</td>\n",
       "      <td>fic_1822_7275.txt-530-41</td>\n",
       "      <td>fic_1971_14051.txt-667-2</td>\n",
       "      <td>afternoon_nn</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 3, 4]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34342</th>\n",
       "      <td>I described my impressions frankly, and with w...</td>\n",
       "      <td>To have this knowledge implies that children h...</td>\n",
       "      <td>96:101</td>\n",
       "      <td>225:230</td>\n",
       "      <td>fic_1853_9200.txt-674-17</td>\n",
       "      <td>nf_1996_774044.txt-1075-37</td>\n",
       "      <td>word_nn</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34343</th>\n",
       "      <td>If they would reflect that this word sovereign...</td>\n",
       "      <td>let himself go to his wife in the words that s...</td>\n",
       "      <td>32:36</td>\n",
       "      <td>34:39</td>\n",
       "      <td>fic_1844_8787.txt-57-6</td>\n",
       "      <td>mag_1963_441107.txt-252-8</td>\n",
       "      <td>word_nn</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34344</th>\n",
       "      <td>If they would reflect that this word sovereign...</td>\n",
       "      <td>a book, by the way, concerning which no adequa...</td>\n",
       "      <td>32:36</td>\n",
       "      <td>49:53</td>\n",
       "      <td>fic_1844_8787.txt-57-6</td>\n",
       "      <td>mag_1857_526498.txt-137-11</td>\n",
       "      <td>word_nn</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34345</th>\n",
       "      <td>I passed among the men with a word of encourag...</td>\n",
       "      <td>I was then taken below, and put into a cot, wh...</td>\n",
       "      <td>30:34</td>\n",
       "      <td>174:179</td>\n",
       "      <td>fic_1835_7143.txt-1235-7</td>\n",
       "      <td>fic_1839_7023.txt-2324-35</td>\n",
       "      <td>word_nn</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34346</th>\n",
       "      <td>About ten minutes later, working on the theory...</td>\n",
       "      <td>Although she often argued with him about their...</td>\n",
       "      <td>61:65</td>\n",
       "      <td>120:125</td>\n",
       "      <td>fic_1984_780068.txt-945-12</td>\n",
       "      <td>fic_1993_53862.txt-4-21</td>\n",
       "      <td>word_nn</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[4, 3, 3]</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5910 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context1  \\\n",
       "28437         I'd landed in early afternoon, local time.   \n",
       "28438         I'd landed in early afternoon, local time.   \n",
       "28439         I'd landed in early afternoon, local time.   \n",
       "28440  Mrs. Daggett, nettled by her sister's hint, ro...   \n",
       "28441  Elvira had just got possession, by stealth, of...   \n",
       "...                                                  ...   \n",
       "34342  I described my impressions frankly, and with w...   \n",
       "34343  If they would reflect that this word sovereign...   \n",
       "34344  If they would reflect that this word sovereign...   \n",
       "34345  I passed among the men with a word of encourag...   \n",
       "34346  About ten minutes later, working on the theory...   \n",
       "\n",
       "                                                context2 index_target_token1  \\\n",
       "28437  Old shopping lists and ticket stubs and wads o...               20:29   \n",
       "28438  @@774723 1 Two Years Earlier // One weekday af...               20:29   \n",
       "28439  afternoon that the Indian bought me, he starte...               20:29   \n",
       "28440  She remained in company with the cows all the ...              91:100   \n",
       "28441  On those afternoons by the sea, we knitted, Sh...             201:210   \n",
       "...                                                  ...                 ...   \n",
       "34342  To have this knowledge implies that children h...              96:101   \n",
       "34343  let himself go to his wife in the words that s...               32:36   \n",
       "34344  a book, by the way, concerning which no adequa...               32:36   \n",
       "34345  I was then taken below, and put into a cot, wh...               30:34   \n",
       "34346  Although she often argued with him about their...               61:65   \n",
       "\n",
       "      index_target_token2                 identifier1  \\\n",
       "28437             104:113   fic_1987_780057.txt-318-5   \n",
       "28438               44:53   fic_1987_780057.txt-318-5   \n",
       "28439                 0:9   fic_1987_780057.txt-318-5   \n",
       "28440               46:55    fic_1822_7275.txt-216-23   \n",
       "28441                9:19    fic_1822_7275.txt-530-41   \n",
       "...                   ...                         ...   \n",
       "34342             225:230    fic_1853_9200.txt-674-17   \n",
       "34343               34:39      fic_1844_8787.txt-57-6   \n",
       "34344               49:53      fic_1844_8787.txt-57-6   \n",
       "34345             174:179    fic_1835_7143.txt-1235-7   \n",
       "34346             120:125  fic_1984_780068.txt-945-12   \n",
       "\n",
       "                      identifier2         lemma median_cleaned  judgments  \\\n",
       "28437   fic_1992_40272.txt-627-18  afternoon_nn            4.0     [4, 4]   \n",
       "28438      nf_2008_774723.txt-0-8  afternoon_nn            4.0     [4, 4]   \n",
       "28439   nf_1849_762578.txt-1803-0  afternoon_nn            4.0     [4, 4]   \n",
       "28440    fic_1850_2310.txt-1321-9  afternoon_nn            4.0  [4, 4, 4]   \n",
       "28441    fic_1971_14051.txt-667-2  afternoon_nn            4.0  [4, 3, 4]   \n",
       "...                           ...           ...            ...        ...   \n",
       "34342  nf_1996_774044.txt-1075-37       word_nn            4.0     [4, 4]   \n",
       "34343   mag_1963_441107.txt-252-8       word_nn            3.0     [3, 3]   \n",
       "34344  mag_1857_526498.txt-137-11       word_nn            3.0     [3, 3]   \n",
       "34345   fic_1839_7023.txt-2324-35       word_nn            4.0     [4, 4]   \n",
       "34346     fic_1993_53862.txt-4-21       word_nn            3.0  [4, 3, 3]   \n",
       "\n",
       "      language  \n",
       "28437  english  \n",
       "28438  english  \n",
       "28439  english  \n",
       "28440  english  \n",
       "28441  english  \n",
       "...        ...  \n",
       "34342  english  \n",
       "34343  english  \n",
       "34344  english  \n",
       "34345  english  \n",
       "34346  english  \n",
       "\n",
       "[5910 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_uses_merged[df_train_uses_merged['language']=='english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YVJJ_f3agrG5"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_train_uses_merged['median_cleaned'] = label_encoder.fit_transform(df_train_uses_merged['median_cleaned'])\n",
    "df_dev_uses_merged['median_cleaned'] = label_encoder.transform(df_dev_uses_merged['median_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DUfshRcSgv1m"
   },
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name=\"FacebookAI/xlm-roberta-base\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = XLMRobertaModel.from_pretrained(model_name)\n",
    "\n",
    "    def truncation_indices(self, target_subword_indices, truncation_tokens_before_target=0.5):\n",
    "        max_tokens = 512\n",
    "        n_target_subtokens = target_subword_indices.count(True)\n",
    "        tokens_before = int((max_tokens - n_target_subtokens) * truncation_tokens_before_target)\n",
    "        tokens_after = max_tokens - tokens_before - n_target_subtokens\n",
    "\n",
    "        lindex_target = target_subword_indices.index(True)\n",
    "        rindex_target = lindex_target + n_target_subtokens\n",
    "        lindex = max(lindex_target - tokens_before, 0)\n",
    "        rindex = rindex_target + tokens_after\n",
    "\n",
    "        return lindex, rindex\n",
    "\n",
    "    def get_target_token_embedding(self, context, index):\n",
    "        start_idx = int(str(index).strip().split(':')[0])\n",
    "        end_idx = int(str(index).strip().split(':')[1])\n",
    "\n",
    "        inputs = self.tokenizer(context, return_tensors=\"pt\",\n",
    "                              return_offsets_mapping=True, add_special_tokens=False)\n",
    "\n",
    "        offset_mapping = inputs['offset_mapping'][0].tolist()\n",
    "        input_ids = inputs['input_ids']\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "        subwords_bool_mask = [\n",
    "            (start <= start_idx < end) or (start < end_idx <= end)\n",
    "            or (start_idx <= start and end <= end_idx)\n",
    "            for start, end in offset_mapping\n",
    "        ]\n",
    "\n",
    "        if len(input_ids[0]) > 512:\n",
    "            lindex, rindex = self.truncation_indices(subwords_bool_mask)\n",
    "            tokens = tokens[lindex:rindex]\n",
    "            input_ids = input_ids[:, lindex:rindex]\n",
    "            subwords_bool_mask = subwords_bool_mask[lindex:rindex]\n",
    "            inputs['input_ids'] = input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs['input_ids'])\n",
    "\n",
    "        target_embeddings = outputs.last_hidden_state[0][subwords_bool_mask]\n",
    "        return target_embeddings.mean(dim=0).numpy()\n",
    "\n",
    "    def generate_embeddings(self, df, file_name):\n",
    "        id2embedding = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            identifier1 = row['identifier1']\n",
    "            identifier2 = row['identifier2']\n",
    "\n",
    "            if identifier1 not in id2embedding:\n",
    "                embedding1 = self.get_target_token_embedding(row['context1'],\n",
    "                                                          row['index_target_token1'])\n",
    "                id2embedding[identifier1] = embedding1\n",
    "\n",
    "            if identifier2 not in id2embedding:\n",
    "                embedding2 = self.get_target_token_embedding(row['context2'],\n",
    "                                                          row['index_target_token2'])\n",
    "                id2embedding[identifier2] = embedding2\n",
    "\n",
    "        np.savez(file_name, **id2embedding)\n",
    "        return id2embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-P_coNRYgyPX"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "#embedding_generator = EmbeddingGenerator()\n",
    "#train_embeddings = embedding_generator.generate_embeddings(df_train_uses_merged,\n",
    "                                                         #'subtask1_train_embeddings.npz')\n",
    "#dev_embeddings = embedding_generator.generate_embeddings(df_dev_uses_merged,\n",
    "                                                       #'subtask1_dev_embeddings.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_A3Kjnmcgv49"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, embeddings, max_len=512, is_test=False):\n",
    "        self.data = data\n",
    "        self.embeddings = self.load_embeddings(embeddings)\n",
    "        self.max_len = max_len\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_embeddings(self, file_name):\n",
    "        \"\"\"Load embeddings from a single .npz file.\"\"\"\n",
    "        try:\n",
    "            loaded_embeddings = np.load(file_name)\n",
    "            embeddings = {identifier: loaded_embeddings[identifier]\n",
    "                        for identifier in loaded_embeddings.files}\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings from {file_name}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        identifier1 = row['identifier1']\n",
    "        identifier2 = row['identifier2']\n",
    "\n",
    "        # Get embeddings with zero padding if not found\n",
    "        embedding1 = self.embeddings.get(identifier1, np.zeros(self.max_len))\n",
    "        embedding2 = self.embeddings.get(identifier2, np.zeros(self.max_len))\n",
    "\n",
    "        # Convert embeddings to tensors\n",
    "        target_embedding1 = torch.tensor(embedding1, dtype=torch.float)\n",
    "        target_embedding2 = torch.tensor(embedding2, dtype=torch.float)\n",
    "\n",
    "        if not self.is_test:\n",
    "            # Get the median_cleaned value as a float for regression\n",
    "            target = torch.tensor(float(row['median_cleaned']), dtype=torch.float)\n",
    "\n",
    "            return {\n",
    "                'target_embedding1': target_embedding1,\n",
    "                'target_embedding2': target_embedding2,\n",
    "                'target': target\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'target_embedding1': target_embedding1,\n",
    "                'target_embedding2': target_embedding2\n",
    "            }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle dictionary outputs.\"\"\"\n",
    "    return {key: default_collate([d[key] for d in batch]) for key in batch[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w0blUHNLg2HD"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data=df_train_uses_merged,\n",
    "                            embeddings='subtask1_train_embeddings.npz',\n",
    "                            max_len=512,\n",
    "                            is_test=False)\n",
    "\n",
    "dev_dataset = CustomDataset(data=df_dev_uses_merged,\n",
    "                          embeddings='subtask1_dev_embeddings.npz',\n",
    "                          max_len=512,\n",
    "                          is_test=False)\n",
    "\n",
    "test_dataset = CustomDataset(data=df_test_uses_merged,\n",
    "                          embeddings='subtask1_test_embeddings.npz',\n",
    "                          max_len=512,\n",
    "                          is_test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mpWTP4UOx8bf"
   },
   "outputs": [],
   "source": [
    "#CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained XLM-RoBERTa model\n",
    "roberta = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim=768,  \n",
    "                 dropout_rate=0.2, \n",
    "                 num_classes=4 \n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.roberta = roberta\n",
    "        \n",
    "        # Calculate the total input size for the classifier\n",
    "        classifier_input_size = embedding_dim * 2 \n",
    "\n",
    "        \n",
    "        # Custom classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(classifier_input_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embedding1, embedding2):\n",
    "        #print(f\"Embedding1 shape: {embedding1.shape}\")\n",
    "        #print(f\"Embedding2 shape: {embedding2.shape}\")\n",
    "        #print(f\"Combined features shape: {combined_features.shape}\")\n",
    "\n",
    "        # Validate input shapes\n",
    "        assert embedding1.size(1) == embedding2.size(1), \"Embeddings must have the same dimension\"\n",
    "        #assert embedding1.size(0) == embedding2.size(0) == combined_features.size(0), \"Batch sizes must match\"\n",
    "\n",
    "        # Concatenate embeddings and combined features\n",
    "        features = torch.cat((embedding1, embedding2), dim=1)\n",
    "        #print(f\"Concatenated features shape: {features.shape}\")\n",
    "\n",
    "        # Pass through classifier\n",
    "        return self.classifier(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self,\n",
    "                 learning_rate=1e-4,\n",
    "                 batch_size=32,\n",
    "                 #num_combined_features=4,\n",
    "                 num_epochs=10,\n",
    "                 dropout_rate=0.2,\n",
    "                 num_classes=4,\n",
    "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "\n",
    "        self.model = XLMRobertaModel(\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_classes=num_classes,\n",
    "            #num_combined_features=num_combined_features\n",
    "        ).to(device)\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = device\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    def train(self, train_dataset, dev_dataset):\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        if dev_dataset:\n",
    "            val_loader = DataLoader(\n",
    "                dev_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            total_train_correct = 0\n",
    "            train_batches = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')\n",
    "\n",
    "            for batch in train_batches:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                target_embedding1 = batch['target_embedding1'].to(self.device)\n",
    "                target_embedding2 = batch['target_embedding2'].to(self.device)\n",
    "                #combined_features = batch['combined_features'].to(self.device)\n",
    "                target = batch['target'].squeeze().to(self.device).long()\n",
    "\n",
    "                logits = self.model(target_embedding1, target_embedding2)\n",
    "                loss = self.criterion(logits, target)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                total_train_correct += (preds == target).sum().item()\n",
    "\n",
    "                train_batches.set_postfix({'train_loss': loss.item()})\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            avg_train_accuracy = total_train_correct / len(train_loader.dataset)  # Accuracy over all data\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "            self.train_accuracies.append(avg_train_accuracy)\n",
    "\n",
    "            if dev_dataset:\n",
    "                val_loss, val_accuracy = self.evaluate(val_loader)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracies.append(val_accuracy)\n",
    "\n",
    "                print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Train Accuracy = {avg_train_accuracy:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(self.model.state_dict(), 'xlm_sole.pt')\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Train Accuracy = {avg_train_accuracy:.4f}')\n",
    "\n",
    "        return self.train_losses, self.val_losses, self.train_accuracies, self.val_accuracies\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                target_embedding1 = batch['target_embedding1'].to(self.device)\n",
    "                target_embedding2 = batch['target_embedding2'].to(self.device)\n",
    "                #combined_features = batch['combined_features'].to(self.device)\n",
    "                target = batch['target'].squeeze().to(self.device).long()\n",
    "\n",
    "                logits = self.model(target_embedding1, target_embedding2)\n",
    "                loss = self.criterion(logits, target)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                total_val_correct += (preds == target).sum().item()\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = total_val_correct / len(val_loader.dataset)  # Accuracy over all data\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "        print(f\"Validation Metrics: Accuracy = {val_accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1-Score = {f1:.4f}\")\n",
    "\n",
    "        return avg_val_loss, val_accuracy\n",
    "\n",
    "    def predict(self, test_dataset, return_probabilities=False):\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                target_embedding1 = batch['target_embedding1'].to(self.device)\n",
    "                target_embedding2 = batch['target_embedding2'].to(self.device)\n",
    "                #combined_features = batch['combined_features'].to(self.device)\n",
    "\n",
    "                logits = self.model(target_embedding1, target_embedding2)\n",
    "\n",
    "                if return_probabilities:\n",
    "                    probs = torch.softmax(logits, dim=1)\n",
    "                    predictions.extend(probs.cpu().numpy())\n",
    "                else:\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a371a16ceeb4b78a6f9e336ef871e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6838, Precision = 0.4158, Recall = 0.3148, F1-Score = 0.3198\n",
      "Epoch 1: Train Loss = 0.8580, Train Accuracy = 0.6634, Val Loss = 0.8737, Val Accuracy = 0.6838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beea284dada04699a1aa30c23829a08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6824, Precision = 0.4210, Recall = 0.3564, F1-Score = 0.3594\n",
      "Epoch 2: Train Loss = 0.7349, Train Accuracy = 0.7034, Val Loss = 0.9006, Val Accuracy = 0.6824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe363b7bdc60430b94163cfd53b0410c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6824, Precision = 0.4262, Recall = 0.3623, F1-Score = 0.3651\n",
      "Epoch 3: Train Loss = 0.6937, Train Accuracy = 0.7196, Val Loss = 0.9194, Val Accuracy = 0.6824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c406865ae9840939fb362ce1254a837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6730, Precision = 0.4048, Recall = 0.3811, F1-Score = 0.3797\n",
      "Epoch 4: Train Loss = 0.6649, Train Accuracy = 0.7320, Val Loss = 0.9432, Val Accuracy = 0.6730\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0257b489cb224d6dbe88961186fdd334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6778, Precision = 0.4123, Recall = 0.3615, F1-Score = 0.3669\n",
      "Epoch 5: Train Loss = 0.6399, Train Accuracy = 0.7427, Val Loss = 0.9757, Val Accuracy = 0.6778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5e645e5f024f92aeb6268e90e0d5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6758, Precision = 0.4046, Recall = 0.3684, F1-Score = 0.3718\n",
      "Epoch 6: Train Loss = 0.6168, Train Accuracy = 0.7541, Val Loss = 0.9958, Val Accuracy = 0.6758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977936e5066a4a8bbbfac3141699def6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6788, Precision = 0.4062, Recall = 0.3744, F1-Score = 0.3736\n",
      "Epoch 7: Train Loss = 0.5955, Train Accuracy = 0.7639, Val Loss = 0.9999, Val Accuracy = 0.6788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea35c69a21348d8a93b79c8ae525b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6733, Precision = 0.4016, Recall = 0.3907, F1-Score = 0.3844\n",
      "Epoch 8: Train Loss = 0.5751, Train Accuracy = 0.7739, Val Loss = 1.0057, Val Accuracy = 0.6733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dd41a67387477e843898eb01daed6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6871, Precision = 0.4231, Recall = 0.3712, F1-Score = 0.3761\n",
      "Epoch 9: Train Loss = 0.5565, Train Accuracy = 0.7817, Val Loss = 1.0561, Val Accuracy = 0.6871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f141a24ef7994527a2943734265c5cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: Accuracy = 0.6802, Precision = 0.4200, Recall = 0.3891, F1-Score = 0.3922\n",
      "Epoch 10: Train Loss = 0.5382, Train Accuracy = 0.7903, Val Loss = 1.0439, Val Accuracy = 0.6802\n"
     ]
    }
   ],
   "source": [
    "learner = Learner(\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=32,\n",
    "    num_epochs=10\n",
    ")\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = learner.train(train_dataset, dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-0QoytKoin9N"
   },
   "outputs": [],
   "source": [
    "predictions = learner.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2-OXgdeAx8bj"
   },
   "outputs": [],
   "source": [
    "#learner.load_state_dict(torch.load('best_adapter_model.pt'))\n",
    "#learner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bWT-u5IFx8bk",
    "outputId": "f8a4e121-ab65-48d9-cf4d-8ddc6674127d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved answer.zip with predictions per language.\n"
     ]
    }
   ],
   "source": [
    "df_test_uses_merged['prediction'] = predictions\n",
    "\n",
    "out_dir = 'answer/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "answer_df = df_test_uses_merged[['identifier1', 'identifier2', 'language', 'prediction']].copy()\n",
    "\n",
    "# Saving files per language\n",
    "for language in answer_df[\"language\"].unique():\n",
    "    df_temp = answer_df[answer_df[\"language\"] == language].drop('language', axis=1)\n",
    "    df_temp['prediction'] = label_encoder.inverse_transform(df_temp['prediction'])\n",
    "    df_temp.to_csv(f'{out_dir}{language}.tsv', index=False, sep='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "\n",
    "with ZipFile('answer.zip', 'w') as zipf:\n",
    "    for root, _, files in os.walk(out_dir):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), arcname=file)\n",
    "\n",
    "print(\"Saved answer.zip with predictions per language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "EtISUiJxjzUx",
    "outputId": "9d26204f-024f-4092-f312-bc1c41fc573e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       "4.0    1226\n",
       "1.0     112\n",
       "2.0       7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "o7jQmh3Wx8bo",
    "outputId": "580dfc60-b46a-46af-f7b4-1db1ada36100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved truth labels for norwegian at ref/norwegian/labels.tsv\n",
      "Saved truth labels for german at ref/german/labels.tsv\n",
      "Saved truth labels for chinese at ref/chinese/labels.tsv\n",
      "Saved truth labels for spanish at ref/spanish/labels.tsv\n",
      "Saved truth labels for english at ref/english/labels.tsv\n",
      "Saved truth labels for russian at ref/russian/labels.tsv\n",
      "Saved truth labels for swedish at ref/swedish/labels.tsv\n"
     ]
    }
   ],
   "source": [
    "def true_labels(df, ref):\n",
    "\n",
    "    if not os.path.exists(ref):\n",
    "        os.makedirs(ref)\n",
    "\n",
    "    languages = df['language'].unique()\n",
    "\n",
    "    for language in languages:\n",
    "        lang_df = df[df['language'] == language][['identifier1', 'identifier2', 'median_cleaned']]\n",
    "\n",
    "        lang_dir = os.path.join(ref, language)\n",
    "        if not os.path.exists(lang_dir):\n",
    "            os.makedirs(lang_dir)\n",
    "\n",
    "        lang_file_path = os.path.join(lang_dir, 'labels.tsv')\n",
    "        lang_df.to_csv(lang_file_path, sep='\\t', index=False, quoting=csv.QUOTE_NONE)\n",
    "        print(f\"Saved truth labels for {language} at {lang_file_path}\")\n",
    "\n",
    "true_labels(df_test_uses_merged, 'ref')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
