{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BYznC9NuNzyk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "import krippendorff\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score,\n",
    "                           precision_recall_fscore_support)\n",
    "from catboost import CatBoostClassifier\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.setup_paths()\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def setup_paths(self):\n",
    "        self.path_dev = 'dev/'\n",
    "        self.path_train = 'train/'\n",
    "        self.path_test = 'test/'\n",
    "        self.path_output = 'answer/'\n",
    "\n",
    "        for path in [self.path_dev, self.path_train, self.path_test, self.path_output]:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "    def extract_zip_files(self):\n",
    "        # Extract dev.zip\n",
    "        if not os.listdir(self.path_dev):\n",
    "            with ZipFile('dev.zip', 'r') as dev:\n",
    "                dev.extractall(self.path_dev)\n",
    "\n",
    "        # Extract train.zip\n",
    "        if not os.listdir(self.path_train):\n",
    "            with ZipFile('train.zip', 'r') as train:\n",
    "                train.extractall(self.path_train)\n",
    "\n",
    "        # Extract test.zip\n",
    "        if not os.listdir(self.path_test):\n",
    "            with ZipFile('test.zip', 'r') as test:\n",
    "                test.extractall(self.path_test)\n",
    "\n",
    "    def load_tsv_files(self):\n",
    "        languages = os.listdir(self.path_train)\n",
    "        self.logger.info(f\"Found languages: {languages}\")\n",
    "\n",
    "        # Initialize file paths\n",
    "        label_file_paths_train = []\n",
    "        uses_file_paths_train = []\n",
    "        label_file_paths_dev = []\n",
    "        uses_file_paths_dev = []\n",
    "        instance_file_paths_test = []\n",
    "        uses_file_paths_test = []\n",
    "\n",
    "        for lang in languages:\n",
    "            label_file_paths_train.append(f\"{self.path_train}{lang}/labels.tsv\")\n",
    "            uses_file_paths_train.append(f\"{self.path_train}{lang}/uses.tsv\")\n",
    "            label_file_paths_dev.append(f\"{self.path_dev}{lang}/labels.tsv\")\n",
    "            uses_file_paths_dev.append(f\"{self.path_dev}{lang}/uses.tsv\")\n",
    "            instance_file_paths_test.append(f\"{self.path_test}{lang}/instances.tsv\")\n",
    "            uses_file_paths_test.append(f\"{self.path_test}{lang}/uses.tsv\")\n",
    "        paths = {\n",
    "            'train_labels_list': label_file_paths_train,\n",
    "            'train_uses_list': uses_file_paths_train,\n",
    "            'dev_labels_list': label_file_paths_dev,\n",
    "            'dev_uses_list': uses_file_paths_dev,\n",
    "            'test_uses_list': uses_file_paths_test,\n",
    "            'test_instances_list': instance_file_paths_test\n",
    "        }\n",
    "\n",
    "        data_dict = {key: [] for key in paths.keys()}\n",
    "\n",
    "        for save_path, path_list in paths.items():\n",
    "            for path in path_list:\n",
    "                with open(path, encoding='utf-8') as tsvfile:\n",
    "                    language = path.split('/')[1]\n",
    "                    reader = csv.DictReader(tsvfile, delimiter='\\t',\n",
    "                                         quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "                    for row in reader:\n",
    "                        row['language'] = language\n",
    "                        data_dict[save_path].append(row)\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "    def create_merged_dataframes(self, data_dict):\n",
    "        def create_mappings(uses_list):\n",
    "            id2context = {}\n",
    "            id2idx = {}\n",
    "            for row in uses_list:\n",
    "                identifier = row['identifier']\n",
    "                id2context[identifier] = row['context']\n",
    "                id2idx[identifier] = row['indices_target_token']\n",
    "            return id2context, id2idx\n",
    "\n",
    "        # Create mappings\n",
    "        train_id2context, train_id2idx = create_mappings(data_dict['train_uses_list'])\n",
    "        dev_id2context, dev_id2idx = create_mappings(data_dict['dev_uses_list'])\n",
    "        test_id2context, test_id2idx = create_mappings(data_dict['test_uses_list'])\n",
    "\n",
    "        # Merge train data\n",
    "        train_uses_merged = []\n",
    "        for row in data_dict['train_labels_list']:\n",
    "            identifier1_train = row['identifier1']\n",
    "            identifier2_train = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': train_id2context.get(identifier1_train),\n",
    "                'context2': train_id2context.get(identifier2_train),\n",
    "                'index_target_token1': train_id2idx.get(identifier1_train),\n",
    "                'index_target_token2': train_id2idx.get(identifier2_train),\n",
    "                'identifier1': identifier1_train,\n",
    "                'identifier2': identifier2_train,\n",
    "                'lemma': row['lemma'],\n",
    "                'median_cleaned': row['median_cleaned'],\n",
    "                'judgments': row['judgments'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            train_uses_merged.append(data_row)\n",
    "\n",
    "\n",
    "        dev_uses_merged = []\n",
    "        for row in data_dict['dev_labels_list']:\n",
    "            identifier1_dev = row['identifier1']\n",
    "            identifier2_dev = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': dev_id2context.get(identifier1_dev),\n",
    "                'context2': dev_id2context.get(identifier2_dev),\n",
    "                'index_target_token1': dev_id2idx.get(identifier1_dev),\n",
    "                'index_target_token2': dev_id2idx.get(identifier2_dev),\n",
    "                'identifier1': identifier1_dev,\n",
    "                'identifier2': identifier2_dev,\n",
    "                'lemma': row['lemma'],\n",
    "                'median_cleaned': row['median_cleaned'],\n",
    "                'judgments': row['judgments'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            dev_uses_merged.append(data_row)\n",
    "\n",
    "        # Merge test data\n",
    "        test_uses_merged = []\n",
    "        for row in data_dict['test_instances_list']:\n",
    "            identifier1_test = row['identifier1']\n",
    "            identifier2_test = row['identifier2']\n",
    "\n",
    "            data_row = {\n",
    "                'context1': test_id2context.get(identifier1_test),\n",
    "                'context2': test_id2context.get(identifier2_test),\n",
    "                'index_target_token1': test_id2idx.get(identifier1_test),\n",
    "                'index_target_token2': test_id2idx.get(identifier2_test),\n",
    "                'identifier1': identifier1_test,\n",
    "                'identifier2': identifier2_test,\n",
    "                'lemma': row['lemma'],\n",
    "                'language': row['language']\n",
    "            }\n",
    "            test_uses_merged.append(data_row)\n",
    "\n",
    "        return pd.DataFrame(train_uses_merged), pd.DataFrame(dev_uses_merged), pd.DataFrame(test_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WCE1lV9tPhlY"
   },
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name=\"FacebookAI/xlm-roberta-base\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = XLMRobertaModel.from_pretrained(model_name)\n",
    "\n",
    "    def truncation_indices(self, target_subword_indices, truncation_tokens_before_target=0.5):\n",
    "        max_tokens = 512\n",
    "        n_target_subtokens = target_subword_indices.count(True)\n",
    "        tokens_before = int((max_tokens - n_target_subtokens) * truncation_tokens_before_target)\n",
    "        tokens_after = max_tokens - tokens_before - n_target_subtokens\n",
    "\n",
    "        lindex_target = target_subword_indices.index(True)\n",
    "        rindex_target = lindex_target + n_target_subtokens\n",
    "        lindex = max(lindex_target - tokens_before, 0)\n",
    "        rindex = rindex_target + tokens_after\n",
    "\n",
    "        return lindex, rindex\n",
    "\n",
    "    def get_target_token_embedding(self, context, index):\n",
    "        start_idx = int(str(index).strip().split(':')[0])\n",
    "        end_idx = int(str(index).strip().split(':')[1])\n",
    "\n",
    "        inputs = self.tokenizer(context, return_tensors=\"pt\",\n",
    "                              return_offsets_mapping=True, add_special_tokens=False)\n",
    "\n",
    "        offset_mapping = inputs['offset_mapping'][0].tolist()\n",
    "        input_ids = inputs['input_ids']\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "        subwords_bool_mask = [\n",
    "            (start <= start_idx < end) or (start < end_idx <= end)\n",
    "            or (start_idx <= start and end <= end_idx)\n",
    "            for start, end in offset_mapping\n",
    "        ]\n",
    "\n",
    "        if len(input_ids[0]) > 512:\n",
    "            lindex, rindex = self.truncation_indices(subwords_bool_mask)\n",
    "            tokens = tokens[lindex:rindex]\n",
    "            input_ids = input_ids[:, lindex:rindex]\n",
    "            subwords_bool_mask = subwords_bool_mask[lindex:rindex]\n",
    "            inputs['input_ids'] = input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs['input_ids'])\n",
    "\n",
    "        target_embeddings = outputs.last_hidden_state[0][subwords_bool_mask]\n",
    "        return target_embeddings.mean(dim=0).numpy()\n",
    "\n",
    "    def generate_embeddings(self, df, file_name):\n",
    "        id2embedding = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            identifier1 = row['identifier1']\n",
    "            identifier2 = row['identifier2']\n",
    "\n",
    "            if identifier1 not in id2embedding:\n",
    "                embedding1 = self.get_target_token_embedding(row['context1'],\n",
    "                                                          row['index_target_token1'])\n",
    "                id2embedding[identifier1] = embedding1\n",
    "\n",
    "            if identifier2 not in id2embedding:\n",
    "                embedding2 = self.get_target_token_embedding(row['context2'],\n",
    "                                                          row['index_target_token2'])\n",
    "                id2embedding[identifier2] = embedding2\n",
    "\n",
    "        np.savez(file_name, **id2embedding)\n",
    "        return id2embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EuJP2_zboisO"
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, class_labels=None, model_dir='saved_models'):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.trained_models = {}\n",
    "        self.model_dir = model_dir\n",
    "        self.class_labels = class_labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    def prepare_data(self, embeddings_file, df):\n",
    "        \"\"\"\n",
    "        Prepare feature vectors from embeddings and dataframe\n",
    "        \"\"\"\n",
    "        loaded_embeddings = np.load(embeddings_file)\n",
    "        features_list = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            embedding1 = loaded_embeddings[row['identifier1']]\n",
    "            embedding2 = loaded_embeddings[row['identifier2']]\n",
    "            features = self.create_features(embedding1, embedding2)\n",
    "            features_list.append(features)\n",
    "\n",
    "        return np.array(features_list)\n",
    "\n",
    "    def create_features(self, embedding1, embedding2):\n",
    "        \"\"\"\n",
    "        Create feature vector from pair of embeddings\n",
    "        \"\"\"\n",
    "        concatenated = np.concatenate([embedding1, embedding2])\n",
    "        difference = embedding1 - embedding2\n",
    "        product = embedding1 * embedding2\n",
    "\n",
    "        cos_sim = np.dot(embedding1, embedding2) / (\n",
    "            np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "        euclidean_dist = np.linalg.norm(embedding1 - embedding2)\n",
    "        manhattan_dist = np.sum(np.abs(embedding1 - embedding2))\n",
    "\n",
    "        return np.concatenate([\n",
    "            concatenated,\n",
    "            difference,\n",
    "            product,\n",
    "            [cos_sim, euclidean_dist, manhattan_dist]\n",
    "        ])\n",
    "\n",
    "    def fit_label_encoder(self, y):\n",
    "        \"\"\"\n",
    "        Fit label encoder to the target values\n",
    "        \"\"\"\n",
    "        self.label_encoder.fit(y)\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "        return self.label_encoder.transform(y)\n",
    "\n",
    "    def calculate_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        Calculate balanced class weights\n",
    "        \"\"\"\n",
    "        counter = Counter(y)\n",
    "        max_samples = max(counter.values())\n",
    "        weights = {cls: max_samples/count for cls, count in counter.items()}\n",
    "        return weights\n",
    "\n",
    "    def calculate_metrics(self, y_true, y_pred, y_prob=None):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive metrics for imbalanced classification\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "            'balanced_accuracy': float(balanced_accuracy_score(y_true, y_pred)),\n",
    "            'macro_f1': float(f1_score(y_true, y_pred, average='macro')),\n",
    "            'weighted_f1': float(f1_score(y_true, y_pred, average='weighted'))\n",
    "        }\n",
    "\n",
    "        # Calculate per-class precision, recall, and f1\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
    "        for i in range(len(precision)):\n",
    "            metrics[f'class_{i}_precision'] = float(precision[i])\n",
    "            metrics[f'class_{i}_recall'] = float(recall[i])\n",
    "            metrics[f'class_{i}_f1'] = float(f1[i])\n",
    "            metrics[f'class_{i}_support'] = int(support[i])\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def save_fold_model(self, model, model_name, fold_num, metrics):\n",
    "        \"\"\"\n",
    "        Save a trained model and its metrics for a specific fold\n",
    "        \"\"\"\n",
    "        fold_dir = os.path.join(self.model_dir, f'fold_{fold_num}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "        model_path = os.path.join(fold_dir, f'{model_name}_model.pkl')\n",
    "        if model_name == 'cat':\n",
    "            model.save_model(model_path)\n",
    "        else:  # xgb\n",
    "            model.save_model(model_path)\n",
    "\n",
    "        metrics_path = os.path.join(fold_dir, f'{model_name}_metrics.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "\n",
    "    def load_fold_model(self, model_name, fold_num):\n",
    "        \"\"\"\n",
    "        Load a model and its metrics for a specific fold\n",
    "        \"\"\"\n",
    "        fold_dir = os.path.join(self.model_dir, f'fold_{fold_num}')\n",
    "        model_path = os.path.join(fold_dir, f'{model_name}_model.pkl')\n",
    "        metrics_path = os.path.join(fold_dir, f'{model_name}_metrics.json')\n",
    "\n",
    "        if model_name == 'cat':\n",
    "            model = CatBoostClassifier()\n",
    "            model.load_model(model_path)\n",
    "        else:  # xgb\n",
    "            model = xgb.XGBClassifier()\n",
    "            model.load_model(model_path)\n",
    "\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        return model, metrics\n",
    "\n",
    "    def train_and_predict(self, X_train, y_train, X_dev, y_dev, X_test):\n",
    "        \"\"\"\n",
    "        Train models and make predictions\n",
    "        \"\"\"\n",
    "        # Transform labels to 0-based indices\n",
    "        y_train_encoded = self.fit_label_encoder(y_train)\n",
    "        y_dev_encoded = self.label_encoder.transform(y_dev)\n",
    "\n",
    "        print(\"Original class distribution:\", Counter(y_train))\n",
    "        print(\"Encoded class distribution:\", Counter(y_train_encoded))\n",
    "        print(\"Original class distribution (Dev):\", Counter(y_dev))\n",
    "        print(\"Encoded class distribution (Dev):\", Counter(y_dev_encoded))\n",
    "\n",
    "        # Calculate class weights using encoded labels\n",
    "        class_weights = self.calculate_class_weights(y_train_encoded)\n",
    "        print(\"Calculated class weights:\", class_weights)\n",
    "\n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_dev_scaled = self.scaler.transform(X_dev)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "\n",
    "        # Save scaler and label encoder\n",
    "        scaler_path = os.path.join(self.model_dir, 'scaler.pkl')\n",
    "        encoder_path = os.path.join(self.model_dir, 'label_encoder.pkl')\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        with open(encoder_path, 'wb') as f:\n",
    "            pickle.dump(self.label_encoder, f)\n",
    "\n",
    "        # Convert class weights to model format\n",
    "        catboost_class_weights = [class_weights[i] for i in range(self.num_classes)]\n",
    "\n",
    "        models = {\n",
    "            'cat': CatBoostClassifier(\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                random_seed=42,\n",
    "                verbose=0,\n",
    "                loss_function='MultiClass',\n",
    "                classes_count=self.num_classes,\n",
    "                class_weights=catboost_class_weights\n",
    "            ),\n",
    "            'xgb': xgb.XGBClassifier(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                objective='multi:softmax',\n",
    "                num_class=self.num_classes,\n",
    "                eval_metric='mlogloss',\n",
    "                scale_pos_weight=class_weights[1]/class_weights[0] if self.num_classes == 2 else 1\n",
    "            )\n",
    "        }\n",
    "\n",
    "        num_split = 3\n",
    "        skf = StratifiedKFold(n_splits=num_split, shuffle=True, random_state=42)\n",
    "        cv_scores = {name: {\n",
    "            'accuracy': [],\n",
    "            'balanced_accuracy': [],\n",
    "            'macro_f1': [],\n",
    "            'weighted_f1': []\n",
    "        } for name in models.keys()}\n",
    "\n",
    "        self.fold_models = {f'fold_{i+1}': {} for i in range(num_split)}\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train_encoded)):\n",
    "            fold_num = fold + 1\n",
    "            X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train_encoded[train_idx], y_train_encoded[val_idx]\n",
    "\n",
    "            print(f\"\\nFold {fold_num}/{num_split}\")\n",
    "            print(f\"Fold {fold_num} class distribution - Training:\", Counter(y_fold_train))\n",
    "            print(f\"Fold {fold_num} class distribution - Validation:\", Counter(y_fold_val))\n",
    "\n",
    "            fold_dir = os.path.join(self.model_dir, f'fold_{fold_num}')\n",
    "            os.makedirs(fold_dir, exist_ok=True)\n",
    "            np.save(os.path.join(fold_dir, 'train_indices.npy'), train_idx)\n",
    "            np.save(os.path.join(fold_dir, 'val_indices.npy'), val_idx)\n",
    "\n",
    "            for name, model in models.items():\n",
    "                print(f\"Training {name}...\")\n",
    "                model.fit(X_fold_train, y_fold_train)\n",
    "                y_pred = model.predict(X_fold_val)\n",
    "                y_prob = model.predict_proba(X_fold_val)\n",
    "\n",
    "                metrics = self.calculate_metrics(y_fold_val, y_pred, y_prob)\n",
    "\n",
    "                for metric in ['accuracy', 'balanced_accuracy', 'macro_f1', 'weighted_f1']:\n",
    "                    cv_scores[name][metric].append(metrics[metric])\n",
    "\n",
    "                self.save_fold_model(model, name, fold_num, metrics)\n",
    "\n",
    "                print(f\"\\n{name} metrics for fold {fold_num}:\")\n",
    "                for metric, value in metrics.items():\n",
    "                    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Save overall CV scores\n",
    "        cv_results = {}\n",
    "        print(\"\\nAverage CV Scores:\")\n",
    "        for name, scores in cv_scores.items():\n",
    "            model_results = {}\n",
    "            for metric, values in scores.items():\n",
    "                mean_value = np.mean(values)\n",
    "                std_value = np.std(values)\n",
    "                model_results[metric] = {\n",
    "                    'mean': float(mean_value),\n",
    "                    'std': float(std_value)\n",
    "                }\n",
    "                print(f\"{name} {metric}: {mean_value:.4f} ± {std_value:.4f}\")\n",
    "            cv_results[name] = model_results\n",
    "\n",
    "        cv_results_path = os.path.join(self.model_dir, 'cv_results.json')\n",
    "        with open(cv_results_path, 'w') as f:\n",
    "            json.dump(cv_results, f, indent=4)\n",
    "\n",
    "        # Train final models\n",
    "        predictions = {}\n",
    "        weights = {'cat': 0.6, 'xgb': 0.4}\n",
    "\n",
    "        final_models_dir = os.path.join(self.model_dir, 'final_models')\n",
    "        os.makedirs(final_models_dir, exist_ok=True)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining final {name} model...\")\n",
    "            model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "            model_path = os.path.join(final_models_dir, f'{name}_final_model.pkl')\n",
    "            if name == 'cat':\n",
    "                model.save_model(model_path)\n",
    "            else:  # xgb\n",
    "                model.save_model(model_path)\n",
    "\n",
    "            predictions[name] = model.predict_proba(X_dev_scaled)\n",
    "\n",
    "        weighted_probas = np.zeros_like(predictions['cat'])\n",
    "        for name, pred_proba in predictions.items():\n",
    "            weighted_probas += weights[name] * pred_proba\n",
    "\n",
    "        final_predictions = np.argmax(weighted_probas, axis=1)\n",
    "\n",
    "        # Transform predictions back to original labels\n",
    "        final_predictions = self.label_encoder.inverse_transform(final_predictions)\n",
    "\n",
    "        return final_predictions, cv_results\n",
    "\n",
    "    def predict_with_fold_model(self, X_test, fold_num, model_name='ensemble'):\n",
    "        \"\"\"\n",
    "        Make predictions using a specific fold's model\n",
    "        \"\"\"\n",
    "        # Load encoders\n",
    "        scaler_path = os.path.join(self.model_dir, 'scaler.pkl')\n",
    "        encoder_path = os.path.join(self.model_dir, 'label_encoder.pkl')\n",
    "\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(encoder_path, 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        if model_name == 'ensemble':\n",
    "            cat_model, _ = self.load_fold_model('cat', fold_num)\n",
    "            xgb_model, _ = self.load_fold_model('xgb', fold_num)\n",
    "\n",
    "            cat_pred = cat_model.predict_proba(X_test_scaled)\n",
    "            xgb_pred = xgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "            weights = {'cat': 0.6, 'xgb': 0.4}\n",
    "            weighted_probas = weights['cat'] * cat_pred + weights['xgb'] * xgb_pred\n",
    "\n",
    "            predictions = np.argmax(weighted_probas, axis=1)\n",
    "        else:\n",
    "            model, _ = self.load_fold_model(model_name, fold_num)\n",
    "            predictions = model.predict(X_test_scaled)\n",
    "\n",
    "        # Transform predictions back to original labels\n",
    "        return self.label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    def predict_with_final_model(self, X_test):\n",
    "        \"\"\"\n",
    "        Make predictions using the final trained models\n",
    "        \"\"\"\n",
    "        # Load encoders\n",
    "        scaler_path = os.path.join(self.model_dir, 'scaler.pkl')\n",
    "        encoder_path = os.path.join(self.model_dir, 'label_encoder.pkl')\n",
    "\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(encoder_path, 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        predictions = {}\n",
    "        weights = {'cat': 0.6, 'xgb': 0.4}\n",
    "        final_models_dir = os.path.join(self.model_dir, 'final_models')\n",
    "\n",
    "        for name in ['cat', 'xgb']:\n",
    "            model_path = os.path.join(final_models_dir, f'{name}_final_model.pkl')\n",
    "            if name == 'cat':\n",
    "                model = CatBoostClassifier()\n",
    "                model.load_model(model_path)\n",
    "            else:  # xgb\n",
    "                model = xgb.XGBClassifier()\n",
    "                model.load_model(model_path)\n",
    "\n",
    "            predictions[name] = model.predict_proba(X_test_scaled)\n",
    "\n",
    "        weighted_probas = np.zeros_like(predictions['cat'])\n",
    "        for name, pred_proba in predictions.items():\n",
    "            weighted_probas += weights[name] * pred_proba\n",
    "\n",
    "        predictions = np.argmax(weighted_probas, axis=1)\n",
    "\n",
    "        # Transform predictions back to original labels\n",
    "        return self.label_encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kbZA_GNcokui"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 01:09:26,375 - INFO - Found languages: ['norwegian', 'german', 'chinese', 'spanish', 'english', 'russian', 'swedish']\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader()\n",
    "data_loader.extract_zip_files()\n",
    "data_dict = data_loader.load_tsv_files()\n",
    "df_train_uses_merged, df_dev_uses_merged, df_test_uses_merged  = data_loader.create_merged_dataframes(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "Tfb2a__8orxb",
    "outputId": "e2191ac5-fdf2-4af4-eab4-0a978a3cffbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "median_cleaned\n",
       "4.0    30257\n",
       "1.0     7099\n",
       "3.0     5967\n",
       "2.0     4510\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_uses_merged['median_cleaned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiHVbmourR84",
    "outputId": "7e915eb8-eae5-499c-b8bd-5daf93960506"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47833, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_uses_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uhuK24ZjrM8y"
   },
   "outputs": [],
   "source": [
    "#embedding_generator = EmbeddingGenerator()\n",
    "#train_embeddings = embedding_generator.generate_embeddings(df_train_uses_merged, 'subtask2_train_embeddings.npz')\n",
    "#dev_embeddings = embedding_generator.generate_embeddings(df_dev_uses_merged, 'subtask2_dev_embeddings.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Bdf0_qudorC-"
   },
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(class_labels=4, model_dir='saved_models')\n",
    "X_train = model_trainer.prepare_data('subtask1_train_embeddings.npz', df_train_uses_merged)\n",
    "X_dev = model_trainer.prepare_data('subtask1_dev_embeddings.npz', df_dev_uses_merged)\n",
    "y_train = df_train_uses_merged['median_cleaned'].astype(float).values\n",
    "y_dev = df_dev_uses_merged['median_cleaned'].astype(float).values\n",
    "X_test = model_trainer.prepare_data('subtask1_test_embeddings.npz', df_test_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({4.0: 30257, 1.0: 7099, 3.0: 5967, 2.0: 4510})\n",
      "Encoded class distribution: Counter({3: 30257, 0: 7099, 2: 5967, 1: 4510})\n",
      "Original class distribution (Dev): Counter({4.0: 5676, 1.0: 1055, 2.0: 817, 3.0: 739})\n",
      "Encoded class distribution (Dev): Counter({3: 5676, 0: 1055, 1: 817, 2: 739})\n",
      "Calculated class weights: {2: 5.070722306016424, 3: 1.0, 0: 4.262149598535005, 1: 6.708869179600887}\n",
      "\n",
      "Fold 1/3\n",
      "Fold 1 class distribution - Training: Counter({3: 20171, 0: 4733, 2: 3978, 1: 3006})\n",
      "Fold 1 class distribution - Validation: Counter({3: 10086, 0: 2366, 2: 1989, 1: 1504})\n",
      "Training cat...\n",
      "\n",
      "cat metrics for fold 1:\n",
      "accuracy: 0.6866\n",
      "balanced_accuracy: 0.6885\n",
      "macro_f1: 0.6149\n",
      "weighted_f1: 0.7049\n",
      "class_0_precision: 0.5362\n",
      "class_0_recall: 0.8178\n",
      "class_0_f1: 0.6477\n",
      "class_0_support: 2366.0000\n",
      "class_1_precision: 0.4504\n",
      "class_1_recall: 0.6941\n",
      "class_1_f1: 0.5463\n",
      "class_1_support: 1504.0000\n",
      "class_2_precision: 0.4167\n",
      "class_2_recall: 0.5631\n",
      "class_2_f1: 0.4789\n",
      "class_2_support: 1989.0000\n",
      "class_3_precision: 0.9344\n",
      "class_3_recall: 0.6791\n",
      "class_3_f1: 0.7865\n",
      "class_3_support: 10086.0000\n",
      "Training xgb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:27:51] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [01:59:36] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xgb metrics for fold 1:\n",
      "accuracy: 0.7985\n",
      "balanced_accuracy: 0.6275\n",
      "macro_f1: 0.6690\n",
      "weighted_f1: 0.7799\n",
      "class_0_precision: 0.7798\n",
      "class_0_recall: 0.7126\n",
      "class_0_f1: 0.7447\n",
      "class_0_support: 2366.0000\n",
      "class_1_precision: 0.7283\n",
      "class_1_recall: 0.5080\n",
      "class_1_f1: 0.5985\n",
      "class_1_support: 1504.0000\n",
      "class_2_precision: 0.6933\n",
      "class_2_recall: 0.3363\n",
      "class_2_f1: 0.4529\n",
      "class_2_support: 1989.0000\n",
      "class_3_precision: 0.8168\n",
      "class_3_recall: 0.9531\n",
      "class_3_f1: 0.8797\n",
      "class_3_support: 10086.0000\n",
      "\n",
      "Fold 2/3\n",
      "Fold 2 class distribution - Training: Counter({3: 20171, 0: 4733, 2: 3978, 1: 3007})\n",
      "Fold 2 class distribution - Validation: Counter({3: 10086, 0: 2366, 2: 1989, 1: 1503})\n",
      "Training cat...\n",
      "\n",
      "cat metrics for fold 2:\n",
      "accuracy: 0.6861\n",
      "balanced_accuracy: 0.6813\n",
      "macro_f1: 0.6119\n",
      "weighted_f1: 0.7043\n",
      "class_0_precision: 0.5401\n",
      "class_0_recall: 0.8052\n",
      "class_0_f1: 0.6465\n",
      "class_0_support: 2366.0000\n",
      "class_1_precision: 0.4457\n",
      "class_1_recall: 0.6693\n",
      "class_1_f1: 0.5351\n",
      "class_1_support: 1503.0000\n",
      "class_2_precision: 0.4139\n",
      "class_2_recall: 0.5666\n",
      "class_2_f1: 0.4784\n",
      "class_2_support: 1989.0000\n",
      "class_3_precision: 0.9279\n",
      "class_3_recall: 0.6842\n",
      "class_3_f1: 0.7877\n",
      "class_3_support: 10086.0000\n",
      "Training xgb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [02:01:24] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [02:30:17] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xgb metrics for fold 2:\n",
      "accuracy: 0.7978\n",
      "balanced_accuracy: 0.6266\n",
      "macro_f1: 0.6690\n",
      "weighted_f1: 0.7798\n",
      "class_0_precision: 0.7974\n",
      "class_0_recall: 0.7054\n",
      "class_0_f1: 0.7486\n",
      "class_0_support: 2366.0000\n",
      "class_1_precision: 0.7072\n",
      "class_1_recall: 0.4997\n",
      "class_1_f1: 0.5856\n",
      "class_1_support: 1503.0000\n",
      "class_2_precision: 0.6885\n",
      "class_2_recall: 0.3489\n",
      "class_2_f1: 0.4631\n",
      "class_2_support: 1989.0000\n",
      "class_3_precision: 0.8154\n",
      "class_3_recall: 0.9524\n",
      "class_3_f1: 0.8786\n",
      "class_3_support: 10086.0000\n",
      "\n",
      "Fold 3/3\n",
      "Fold 3 class distribution - Training: Counter({3: 20172, 0: 4732, 2: 3978, 1: 3007})\n",
      "Fold 3 class distribution - Validation: Counter({3: 10085, 0: 2367, 2: 1989, 1: 1503})\n",
      "Training cat...\n",
      "\n",
      "cat metrics for fold 3:\n",
      "accuracy: 0.6925\n",
      "balanced_accuracy: 0.6802\n",
      "macro_f1: 0.6159\n",
      "weighted_f1: 0.7094\n",
      "class_0_precision: 0.5496\n",
      "class_0_recall: 0.8099\n",
      "class_0_f1: 0.6548\n",
      "class_0_support: 2367.0000\n",
      "class_1_precision: 0.4640\n",
      "class_1_recall: 0.6653\n",
      "class_1_f1: 0.5467\n",
      "class_1_support: 1503.0000\n",
      "class_2_precision: 0.4085\n",
      "class_2_recall: 0.5480\n",
      "class_2_f1: 0.4681\n",
      "class_2_support: 1989.0000\n",
      "class_3_precision: 0.9217\n",
      "class_3_recall: 0.6976\n",
      "class_3_f1: 0.7941\n",
      "class_3_support: 10085.0000\n",
      "Training xgb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [02:32:05] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [03:01:39] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xgb metrics for fold 3:\n",
      "accuracy: 0.7968\n",
      "balanced_accuracy: 0.6160\n",
      "macro_f1: 0.6632\n",
      "weighted_f1: 0.7769\n",
      "class_0_precision: 0.8063\n",
      "class_0_recall: 0.6929\n",
      "class_0_f1: 0.7453\n",
      "class_0_support: 2367.0000\n",
      "class_1_precision: 0.7229\n",
      "class_1_recall: 0.4704\n",
      "class_1_f1: 0.5699\n",
      "class_1_support: 1503.0000\n",
      "class_2_precision: 0.7063\n",
      "class_2_recall: 0.3409\n",
      "class_2_f1: 0.4598\n",
      "class_2_support: 1989.0000\n",
      "class_3_precision: 0.8085\n",
      "class_3_recall: 0.9597\n",
      "class_3_f1: 0.8776\n",
      "class_3_support: 10085.0000\n",
      "\n",
      "Average CV Scores:\n",
      "cat accuracy: 0.6884 ± 0.0029\n",
      "cat balanced_accuracy: 0.6834 ± 0.0037\n",
      "cat macro_f1: 0.6142 ± 0.0017\n",
      "cat weighted_f1: 0.7062 ± 0.0023\n",
      "xgb accuracy: 0.7977 ± 0.0007\n",
      "xgb balanced_accuracy: 0.6234 ± 0.0052\n",
      "xgb macro_f1: 0.6670 ± 0.0027\n",
      "xgb weighted_f1: 0.7789 ± 0.0014\n",
      "\n",
      "Training final cat model...\n",
      "\n",
      "Training final xgb model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [03:03:47] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/sarumio/miniconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [03:36:18] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "predictions, cv_results = model_trainer.train_and_predict(X_train, y_train, X_dev, y_dev, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_predictions = model_trainer.predict_with_fold_model(X_train, fold_num=1, model_name='ensemble')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "rxyAPv32opLT",
    "outputId": "48279aba-9e14-4038-eb8f-e76ed0c52b4f"
   },
   "outputs": [],
   "source": [
    "#predictions, cv_results = model_trainer.train_and_predict(X_train, y_train, X_train)\n",
    "# Make predictions using a specific fold's model\n",
    "#fold_predictions = model_trainer.predict_with_fold_model(X_train, fold_num=1, model_name='ensemble')\n",
    "# Make predictions using the final model\n",
    "final_predictions = model_trainer.predict_with_final_model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data by language\n",
    "for language, group in df_test_uses_merged.groupby('language'):\n",
    "    test_indices = group.index\n",
    "    X_test_ = X_test[test_indices]  # Select test data for the specific language\n",
    "    y_pred = model_trainer.predict_with_fold_model(X_test_, fold_num=1, model_name='ensemble')  \n",
    "    df_test_uses_merged.loc[test_indices, 'prediction'] = y_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MqTffTf09GuR"
   },
   "outputs": [],
   "source": [
    "# Create answer file in required format for codalab\n",
    "out_dir = 'answer/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "answer_df = df_test_uses_merged[['identifier1', 'identifier2', 'prediction', 'language']]\n",
    "answer_df = answer_df.reset_index(drop=True)\n",
    "for i in list(answer_df[\"language\"].value_counts().index):\n",
    "    df_temp = answer_df[answer_df[\"language\"]==i]\n",
    "    df_temp = df_temp.drop('language', axis=1)\n",
    "    df_temp.to_csv(f'{out_dir}{i}.tsv', index=False, sep='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "\n",
    "with ZipFile('answer.zip', 'w') as zipf:\n",
    "    for root, _, files in os.walk(out_dir):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), arcname=file)\n",
    "\n",
    "answer_df.to_csv(\"prediction_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
